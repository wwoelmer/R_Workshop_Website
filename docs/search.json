[
  {
    "objectID": "workshops/R_Tutorial_6_2025.html",
    "href": "workshops/R_Tutorial_6_2025.html",
    "title": "Lesson 6: Trend Analysis",
    "section": "",
    "text": "This lesson is designed to introduce you to the ‘LWPTrends’ package. This package is an internal package but has been built around functions developed by Ton Snelder and Caroline Fraser from LWP. Additional functions have been developed by me (James Dare) to make the LWP functions more applicable to BOPRC data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\noutliers\nplyr\nggpubr\nNADA\ngam\nconflicted\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nWe will also use an internal package:\n\n\n\nLWPTrends will need to be installed manually from a tar file:\n\ninstall.packages(\"V:/Applications/R Course/Packages/LWPTrends_1.0.0.tar.gz\", repos = NULL,\n    type = \"source\")\n\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\nlibrary(outliers)\nlibrary(plyr)\nlibrary(ggpubr)\nlibrary(NADA)\nlibrary(gam)\nlibrary(LWPTrends)\nlibrary(conflicted)\n\nFor those that are new to R, ‘plyr’ is the old version of dplyr, which is part of the tidyverse. Unfortunately, the LWPTrends package relies heavily on ‘plyr’ so we have to use it, however tidyverse uses the newer version, ‘dplyr’. This causes problems because both packages have functions with the same name, yet operate in different ways (remember that conversation we had on the first day).\nWe will manage this problem using the ‘conflicted’ package. This package allows us to tell R to prefer the ‘dplyr’ versions of these functions over the ‘plyr’ versions. There are only seven conflicted functions so it shouldn’t affect the functionality of the ‘LWPTrends’ package.\n\nconflict_prefer(\"summarise\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"arrange\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"rename\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"summarise\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nToday we will be looking at two different datasets which we will use to run two different variants of trend analysis. We will start with a simple example of annual MCI data from ‘Waiari at Te Puke Highway’, before moving on to a monthly NNN dataset from Rangitaiki at Te Teko. We will use the latter to demonstrate seasonal and co-variate (flow) adjustment. Both datasets can be found in the ‘Datasets’ folder."
  },
  {
    "objectID": "workshops/R_Tutorial_6_2025.html#overview",
    "href": "workshops/R_Tutorial_6_2025.html#overview",
    "title": "Lesson 6: Trend Analysis",
    "section": "",
    "text": "This lesson is designed to introduce you to the ‘LWPTrends’ package. This package is an internal package but has been built around functions developed by Ton Snelder and Caroline Fraser from LWP. Additional functions have been developed by me (James Dare) to make the LWP functions more applicable to BOPRC data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\noutliers\nplyr\nggpubr\nNADA\ngam\nconflicted\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nWe will also use an internal package:\n\n\n\nLWPTrends will need to be installed manually from a tar file:\n\ninstall.packages(\"V:/Applications/R Course/Packages/LWPTrends_1.0.0.tar.gz\", repos = NULL,\n    type = \"source\")\n\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\nlibrary(outliers)\nlibrary(plyr)\nlibrary(ggpubr)\nlibrary(NADA)\nlibrary(gam)\nlibrary(LWPTrends)\nlibrary(conflicted)\n\nFor those that are new to R, ‘plyr’ is the old version of dplyr, which is part of the tidyverse. Unfortunately, the LWPTrends package relies heavily on ‘plyr’ so we have to use it, however tidyverse uses the newer version, ‘dplyr’. This causes problems because both packages have functions with the same name, yet operate in different ways (remember that conversation we had on the first day).\nWe will manage this problem using the ‘conflicted’ package. This package allows us to tell R to prefer the ‘dplyr’ versions of these functions over the ‘plyr’ versions. There are only seven conflicted functions so it shouldn’t affect the functionality of the ‘LWPTrends’ package.\n\nconflict_prefer(\"summarise\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"arrange\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"rename\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"summarise\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nToday we will be looking at two different datasets which we will use to run two different variants of trend analysis. We will start with a simple example of annual MCI data from ‘Waiari at Te Puke Highway’, before moving on to a monthly NNN dataset from Rangitaiki at Te Teko. We will use the latter to demonstrate seasonal and co-variate (flow) adjustment. Both datasets can be found in the ‘Datasets’ folder."
  },
  {
    "objectID": "workshops/R_Tutorial_6_2025.html#macroinvertebrate-data",
    "href": "workshops/R_Tutorial_6_2025.html#macroinvertebrate-data",
    "title": "Lesson 6: Trend Analysis",
    "section": "2 Macroinvertebrate Data",
    "text": "2 Macroinvertebrate Data\nLet’s start by loading the macroinvertebrate dataset:\n\nMCI_Data &lt;- read.csv(\"./data/MCI_Waiari.csv\")\n\nOpen up the dataset by double clicking on the ‘MCI_Data’ object in the data panel (upper right corner). Notice that there are no date or time stamps. We only have a ‘Period’ to work with. Trend analysis requires a time stamp, so let’s convert the period into a date where we assume the sample is collected on the 1st January each year. First we need to create a column called ‘Year’ which takes the year component from the ‘Period’ string. The substring function allows you to subset a string, you just need to provide a point to start and end. In this case, we want to start at the 1st character and end at the 4th character.\n\nMCI_Data &lt;- MCI_Data %&gt;%\n    mutate(Year = substring(Period, 1, 4))\n\nNow we need to convert the year into a date object where the day and month will be ‘01-01’ i.e., the 1st of January.\n\nChallenge 1: Create another column called ‘myDate’ which is equal to the column ‘Year’ combined with ‘01-01’. Hint - you will need to do this in two steps. First create a string using the ‘paste0’ combined with ‘01-01’. Secondly, convert that string to a Date using as.Date(). You can do this in one line of code. Finally, select the columns: ‘Aquarius._SIteID’,‘myDate’,‘MCI’, and create a new column called ‘analyte’ which is equal to “MCI”.\n\n\nClick to see a solution\n\n\nMCI_Data &lt;- MCI_Data %&gt;%\n    mutate(myDate = as.Date(paste0(Year, \"-01-01\"), tz = \"etc/GMT+12\")) %&gt;%\n    select(Aquarius._SIteID, myDate, MCI) %&gt;%\n    mutate(analyte = \"MCI\")\n\n\n\nGreat. Look at your dataset and you should have four columns: ‘Aquarius._SIteID’,‘myDate’,‘MCI’, and ‘analyte’. The LWPTrends package requires specific names for some columns in order for functions to work. The timestamp should be in a date format called ‘myDate’ and the parameter should be called ‘analyte’.\nNow we have to prepare the dataset for trend analysis. This is set out in seven repeatable steps which each involve a specific LWPTrends function.\n\n\n\nAppend date information to the dataset.\n\nThe first function is ‘GetMoreDateInfo()’. This adds on the necessary information that allows steps 3-6 to determine which year, month, and quarter each date relates to.\n\n#Add on time increment and extra date information - NOTE if this dataset was is based on Water Years firstMonth = 7\nMCI_Data &lt;- GetMoreDateInfo(MCI_Data)\n\n\n\n\nProcess censored values.\n\nNow we need to apply the ‘RemoveAlphaDetect()’ function, which takes a dataframe that might include censored values (specified as the prefix &gt; or &lt;,column is type character) and returns face values and information about the nature of the censoring.\nIn this case, our data is uncensored, but we still need to run this function to move to the next stage.\n\n#Process censored values\nMCI_Data &lt;- RemoveAlphaDetect(MCI_Data,ColToUse=\"MCI\")\n\nThis function will provide a new column ‘RawValue’ with the raw data, alongside two additional columns that detail whether each value was censored and what type of censor it was. This information is passed to the trend analysis function in future steps.\nThe LWPTrends package handles censored values in two ways. For point statistics such as means, standard deviations or quantiles, left censored values are imputed using the regression on order statistics (ROS) method, while right censored values are imputed using the survreg method. Censored values used to calculate Kendall’s S and its p-value are handled in the manner recommended by Helsel (2005, 2012). Refer to the LWPTrends help document for more information.\n\n\n\nInspect the data.\n\nNow it is time to inspect our dataset using ‘InspectTrendData()’. This function will provide important diagnostic output, while also appending the dataset with additional columns that are necessary to continue with trend analysis. There are a number of arguments that can be input to the InspectTrendData() function, but the one that we need to worry about is the ‘EndYear’, ‘propYearTol’, and ‘propIncrTol’. End year, is just the year that you want the analysis to end. This is required, but can also be used in conjunction with ‘TrendPeriod’ to shorten the dataset. The ‘propYearTol’, and ‘propIncrTol’ refer to the acceptable proportion of years and time increments that must have observations. The default for these values is 0.9, i.e., 90% of years and time increments must have values.\nLet’s run the data inspection using these defaults to see what happens.\n\nMCI_Inspect &lt;- InspectTrendData(MCI_Data,\n                                EndYear = max(MCI_Data$Year),\n                                ReturnALLincr=TRUE,\n                                do.plot = TRUE,\n                                mymain=\"Example 1\",\n                                UseMidObs=TRUE,\n                                Year=\"Year\",\n                                propYearTol = 0.9,\n                                propIncrTol = 0.9)\n\nThe InspectTrendData object contains three sub-objects, stored in a special type of object class called a ‘list’. You can access sub-objects using the syntax ‘ListName[[SubList_Number]]’. The first sub-object is an amended dataframe, the second is the output of an analysis that determines the best time increment to run trend analysis on, and the third produces a series of diagnostic plots.\nWe need to look at the second sub-list object.\n\nMCI_Inspect[[2]]\n\n     Incr TrendPeriodL nobs nYear  propYear nIncrYear propIncrYear propCen\n1   Month           21   18    18 0.8571429        18   0.07142857       0\n2 BiMonth           21   18    18 0.8571429        18   0.14285714       0\n3     Qtr           21   18    18 0.8571429        18   0.21428571       0\n4   BiAnn           21   18    18 0.8571429        18   0.42857143       0\n5    Year           21   18    18 0.8571429        18   0.85714286       0\n  nCenLevelsLT nCenLevelsGT nFlow DataOK\n1            0            0     0  FALSE\n2            0            0     0  FALSE\n3            0            0     0  FALSE\n4            0            0     0  FALSE\n5            0            0     0  FALSE\n\n\nThis output shows the possible time increments in the leftmost column, followed by a range of other column outputs. Notice the final column ‘DataOK’ states FALSE for all time increments, which is essentially saying that we don’t have enough data to satisfy any of the condtions for any time increment. Take a look at the ‘propYear’ and ‘propIncrYear’ columns and not the values. Any thoughts on why his might have failed?\nThis might make more sense if we look at the diagnostic plots. The complicated looking ‘ggarrange’ call below is complicated because it is calling sub-list components directly rather than saving them as different objects (e.g. plot1, plot2, plot3 etc.). It is also nesting a three-plot ggarrange object as the second row of another ggplot object. This allows you to see a large version of the time series data, and have smaller versions of the matrix plots. This comes directly from Ton and Caroline (don’t blame me..), but I think it makes a very handy diagnostic dashboard.\n\n\n\nggarrange(MCI_Inspect[[3]][[1]],\n          ggarrange(MCI_Inspect[[3]][[2]],\n                    MCI_Inspect[[3]][[3]],\n                    MCI_Inspect[[3]][[4]],nrow=1,align=\"h\"),nrow=2)\n\n\n\n\n\n\n\n\nHopefully you can see that we have gaps in 2009, 2011, and 2012. These gaps mean that we only have observations for 85% of years and 85% of time increments if we run an annual trend analysis.\nWe have two options, either we reduce the trend analysis period to say 2013-2022, or we make proportion of observations requirement more lenient. Let’s do the latter.\n\nChallenge 2: Add ‘Change the propYearTol and propInrTol requirements to 0.8 in the InspectTrendData function. Make sure this is defined as an object named ’MCI_Inspect’. Look at the second sub-list object to see if anything has changed\n\n\nClick to see a solution\n\n\nMCI_Inspect &lt;- InspectTrendData(MCI_Data,\n                                EndYear = max(MCI_Data$Year),\n                                ReturnALLincr=TRUE,\n                                do.plot = TRUE,\n                                mymain=\"Example 1\",\n                                UseMidObs=TRUE,\n                                Year=\"Year\",\n                                propYearTol = 0.8,\n                                propIncrTol = 0.8)\n\nMCI_Inspect[[2]]\n\n     Incr TrendPeriodL nobs nYear  propYear nIncrYear propIncrYear propCen\n1   Month           21   18    18 0.8571429        18   0.07142857       0\n2 BiMonth           21   18    18 0.8571429        18   0.14285714       0\n3     Qtr           21   18    18 0.8571429        18   0.21428571       0\n4   BiAnn           21   18    18 0.8571429        18   0.42857143       0\n5    Year           21   18    18 0.8571429        18   0.85714286       0\n  nCenLevelsLT nCenLevelsGT nFlow DataOK\n1            0            0     0  FALSE\n2            0            0     0  FALSE\n3            0            0     0  FALSE\n4            0            0     0  FALSE\n5            0            0     0   TRUE\n\n\n\n\nHooray, the ‘Year’ time increment now has ‘TRUE’ in the ‘DataOK’ column. We can move on, but first we need to save the appended datasets for the next step.\n\nMCI_Data &lt;- MCI_Inspect[[1]]\n\n\nCheck for seasonality\n\nWe can skip this step given that we are using annual data, i.e., there are no seasons.\n\nDefine a co-variate and test relationship.\n\nAgain, we can skip this part given that we will not be adjusting our data based on a co-variate. We will explore this with the WQ dataset.\n\nRun trend analysis.\n\nWe know that there is no seasonality in our data so we will use the function ‘NonSeasonalTrendAnalysis()’. Again, this produces a list with two sub-list items. The first is the trend analysis results, and the second is a plot that shows you the trend analysis output in graphical form. The graph is shown below.\n\nMCI_Trend_Analysis_Output&lt;-NonSeasonalTrendAnalysis(MCI_Data,mymain=\"Ex 1 Raw Trend\",do.plot=T)\n\nMCI_Trend_Analysis_Output[[2]]\n\n\n\n\n\n\n\n\n\nClassify the output.\n\nWe can see that the trend is decreasing over time, but does this mean that things are improving or degrading and what confidence do we have around this? The ‘AssignConfCat()’ function can classify the trend output in a number of different ways, ranging from full IPCC categories to simplified LAWA categories. We are more familiar with the latter so we will use the simple method.\nTrend direction means different things to different analytes. For example, an incresing trend for nitrate might be a bad thing, but an increasing trend for clarity might be a good thing. Therefore we need to tell this function what analyte we just analysed, and what analytes should be interpreted in reverse. The default reverse analytes for this function are ‘MCI’ and ‘CLAR’, which means that we just need to tell the function that we are dealing with MCI. However, for clarity we will include ‘Reverse’ in the function arguments. Again, it is looking for the ‘analyte’ column, which has dropped off in the trend analysis process and needs to be recreated.\nWe will use this function to create a new column called ‘Direction’ which will contain the trend analysis category.\n\nMCI_Trend_Analysis_Output &lt;- MCI_Trend_Analysis_Output[[1]]\nMCI_Trend_Analysis_Output$analyte &lt;- \"MCI\"\n\nMCI_Trend_Analysis_Output &lt;-MCI_Trend_Analysis_Output %&gt;% \n  mutate(Direction = AssignConfCat(MCI_Trend_Analysis_Output,CatType=\"Improve\",Reverse=c(\"VC\",\"MCI\")))\n\nMCI_Trend_Analysis_Output\n\n  nObs nTimeIncr   S VarS   D        tau         Z          p         C\n1   18        18 -83  697 153 -0.5424837 -3.105971 0.00189655 0.9990517\n         Cd prop.censored prop.unique no.censorlevels TimeIncr    SeasIncr\n1 0.9990517             0           1               0   Annual NonSeasonal\n    Median AnnualSenSlope   Sen_Lci    Sen_Uci AnalysisNote\n1 108.7808      -1.266876 -1.938812 -0.6352487           ok\n  Percent.annual.change TrendDirection analyte             Direction\n1             -1.164614     Decreasing     MCI Very likely degrading\n\n\nLook in the last column and we can see that the trend is ‘Very likely degrading’.\nCongratulations, you have just completed your first trend analysis using the LWPTrends package. Now for something a bit more challenging."
  },
  {
    "objectID": "workshops/R_Tutorial_6_2025.html#river-water-quality-data",
    "href": "workshops/R_Tutorial_6_2025.html#river-water-quality-data",
    "title": "Lesson 6: Trend Analysis",
    "section": "3 River Water Quality Data",
    "text": "3 River Water Quality Data\nThe next dataset contains four analytes (TN, NNN, TP, and DRP) collected from two sites (Rangitaiki at SH5 and Rangitaiki at Te Teko) on the Rangitaiki River.\n\nChallenge 3: Load the ‘DF_for_Trends.csv’ dataset and ensure that the Time column is formatted as a timestamp. \n\n\nClick to see a solution\n\n\nWQ_Data &lt;- read.csv(\"./data/DF_for_Trends.csv\")\n\nWQ_Data &lt;- WQ_Data %&gt;%\n  mutate(Time = parse_date_time(Time, orders = c(\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d\"),tz=\"etc/GMT+12\"))\n\n\n\nLet’s view the data so we can see what we are dealing with.\n\nChallenge 4: Create a ggplot where x=Time and y=Value and facet this by ‘LocationName + analyte’ so we can look at all variables. You can also add ‘colour=analyte’ for geom_point if you wish.\n\n\nClick to see a solution\n\n\nWQ_Data %&gt;%\n  ggplot(aes(x=Time, y=Value))+\n  geom_point(aes(colour=analyte))+\n  theme_bw()+\n  facet_wrap(~LocationName+analyte,scales=\"free\")\n\n\n\n\n\n\n\n\n\n\nWe can also look at which points have associated discharge measurements and if discharge seems to be increasing over time.\n\nWQ_Data %&gt;%\n  ggplot(aes(x=Time, y=Value))+\n  geom_point(aes(colour=Discharge))+\n  theme_bw()+\n  facet_wrap(~LocationName+analyte,scales=\"free\")\n\n\n\n\n\n\n\n\nWe might also decide that we want to remove outliers. Outliers are difficuly to remove manually, but fortunately there is a package called ‘outliers’ that can help. The function of interest within the outliers package is called ‘scores()’ but this requires a fair bit of data manipulation to adapt to our datasets.\nBut don’t fret, I have developed a function called ‘Outliers_Z_Score()’ which does all the hard work for you. All you need to do is input your dataframe in the form of “LocationName”, “Site”, “Time”, “analyte”, “Value”, and set the probability that you are happy with. I called this function ‘Outliers_Z_Score’ but you actually use other methods as well, just set the ‘type’ input to one of the following: (“z”, “t”, “chisq”, “mad”). This function can either return a dataframe of ‘outliers’ or a dataframe of non-outlier, i.e., ‘values’.\nWe will use a probability of 0.99 and use a z score to define outliers in our dataset. We can then compare this to our original dataset and create a new variable that defines which values are outliers.\n\n#run the outliers function.  This creates a dataset of outlier values\nValue_Output &lt;- Outliers_Z_Score(dataset=WQ_Data %&gt;% select(\"LocationName\", \"Site\", \"Time\",  \"analyte\", \"Value\"),\n                                 probability = 0.99,output = \"outliers\",type=\"z\")\n\n#create a unique column so we can compare this to our original dataset.\nValue_Output &lt;-Value_Output %&gt;%\n  mutate(Unique = paste0(Site,Time,Parameter))\n\n#create a variable in the original dataset so we can identifiy our outliers.\nWQ_Data &lt;- WQ_Data %&gt;%\n  mutate(Unique = paste0(Site,Time,analyte)) %&gt;%\n  mutate(Outlier = ifelse(Unique %in% Value_Output$Unique, TRUE, FALSE)) %&gt;%\n  select(Site, LocationName,Time,analyte,Value,Discharge,Outlier)\n\n#create a plot so we can visualise our outliers.\nWQ_Data%&gt;%\n  ggplot(aes(x=Time, y=Value))+\n  geom_point(aes(colour=Outlier))+\n  theme_bw()+\n  facet_wrap(~LocationName+analyte,scales=\"free\")\n\n\n\n\n\n\n\n\nThe blue dots in the figure above show which points have been identified as an outlier.\n\nChallenge 5: Try other outlier detection methods by changing the ‘type’ argument. When you’re happy, use filter() to remove these outliers from the dataset. Make sure you re-define the output as WQ_Data\n\n\nClick to see a solution\n\n\n#run the outliers function.  This creates a dataset of outlier values\nValue_Output &lt;- Outliers_Z_Score(dataset=WQ_Data %&gt;% select(\"LocationName\", \"Site\", \"Time\",  \"analyte\", \"Value\"),\n                                 probability = 0.99,output = \"outliers\",type=\"chi\")\n\n#create a unique column so we can compare this to our original dataset.\nValue_Output &lt;-Value_Output %&gt;%\n  mutate(Unique = paste0(Site,Time,Parameter))\n\n#create a variable in the original dataset so we can identifiy our outliers.\nWQ_Data &lt;- WQ_Data %&gt;%\n  mutate(Unique = paste0(Site,Time,analyte)) %&gt;%\n  mutate(Outlier = ifelse(Unique %in% Value_Output$Unique, TRUE, FALSE)) %&gt;%\n  select(Site, LocationName,Time,analyte,Value,Discharge,Outlier)\n\n#create a plot so we can visualise our outliers.\nWQ_Data%&gt;%\n  ggplot(aes(x=Time, y=Value))+\n  geom_point(aes(colour=Outlier))+\n  theme_bw()+\n  facet_wrap(~LocationName+analyte,scales=\"free\")\n\n\n\n\n\n\n\nWQ_Data &lt;- WQ_Data %&gt;%\n  filter(Outlier == FALSE) %&gt;%\n  select(Site, LocationName,Time,analyte,Value,Discharge)\n\n\n\nOkay, good job. To make things a bit simpler for the next part we will only look at one site and one variable. We will come back to look at the other sites and variable in part 2 of this lesson when we learn how to batch process trends.\nLet’s work with NNN at Rangitaiki at Te Teko.\n\nNNN_Te_Teko &lt;- WQ_Data %&gt;% \n  filter(LocationName == \"Rangitaiki at Te Teko\") %&gt;% \n  filter(analyte == \"NNN (g/m^3)\")\n\nNow we can begin the same process as before:\n\n\n\nAppend date information to the dataset.\nProcess censored values.\n\n\nChallenge 6: See if you can complete steps 1 and 2 for the NNN_Te_Teko dataset. Remember, the timestamp needs to be converted to a date with the name ‘myDate’\n\n\nClick to see a solution\n\n\nNNN_Te_Teko &lt;- NNN_Te_Teko %&gt;% \n  mutate(myDate = as.Date(Time,tz=\"etc/GMT+12\")) %&gt;% \n  GetMoreDateInfo() %&gt;% \n  RemoveAlphaDetect(ColToUse=\"Value\")\n\n\n\n\nInspect the data.\n\nOkay, now lets inspect the data to see what we are dealing with.\n\nInspect_Output_NNN_Te_Teko &lt;- InspectTrendData(NNN_Te_Teko, EndYear = max(NNN_Te_Teko$Year),\n                                           ReturnALLincr=TRUE,do.plot = TRUE,mymain=\"Example 1\",UseMidObs=TRUE)\n\n#note the same nested structure.\nggarrange(Inspect_Output_NNN_Te_Teko[[3]][[1]],\n          ggarrange(Inspect_Output_NNN_Te_Teko[[3]][[2]],\n                    Inspect_Output_NNN_Te_Teko[[3]][[3]],\n                    Inspect_Output_NNN_Te_Teko[[3]][[4]],nrow=1,align=\"h\"),nrow=2)\n\n\n\n\n\n\n\n\nOkay. We can see that we have a few gaps and a few duplicates but nothing too bad. LWPTrends handles duplicate values within each time increment by taking the value that is closest to the mid-point of that increment. You can also tell it to take the median if you want.\nIf we look at the second sub-list item we can see that all time increments are satisfied. The output will allocate the time increment as the most frequent increment that satisfies all requirements, in this case ‘month’.\n\nInspect_Output_NNN_Te_Teko[[2]]\n\n     Incr TrendPeriodL nobs nYear propYear nIncrYear propIncrYear propCen\n1   Month           37  826    37        1       420    0.9459459       0\n2 BiMonth           37  826    37        1       215    0.9684685       0\n3     Qtr           37  826    37        1       145    0.9797297       0\n4   BiAnn           37  826    37        1        73    0.9864865       0\n  nCenLevelsLT nCenLevelsGT nFlow DataOK\n1            0            0     0   TRUE\n2            0            0     0   TRUE\n3            0            0     0   TRUE\n4            0            0     0   TRUE\n\n\nLet’s take the appended dataset created by the InspectTrendData function (sub-list item 1) and move to step 4.\n\nNNN_Te_Teko &lt;- Inspect_Output_NNN_Te_Teko[[1]]\n\n\n\n\nCheck for seasonality.\n\nOur time increment for NNN_Te_Teko is monthly so it’s highly likely there could be some seasonality in the data (i.e., some months could have higher concentrations than others due to climatic or anthropogenic factors). If the data are seasonal, we need to use a different trend analysis function than for non-seasonal data. But how do we determine if the data are seasonal? LWPTrends has a function for that called ‘GetSeason()’. GetSeason performs a Kruskal Wallis (non-parametric) test on the observations using the time increment as the explainatory variable. Use of this function is simple, you just need to input the dataset and tell it which column to look at.\n\nSeason_Output&lt;-GetSeason(NNN_Te_Teko,ValuesToUse = \"RawValue\",mymain=\"Example 1\",do.plot = TRUE)\n\nThis function outputs another list object with three sub-list components: 1) an appended dataset, 2) the output from the Kruskal-Wallis test, and 3) a graph showing the output.\n\nChallenge 7: Look at sub-list components 2 and 3 and come to a conclusion of whether the data is seasonal or not.\n\n\nClick to see a solution\n\n\nSeason_Output[[2]]\n\n                           Observations   KWstat       pvalue SeasNote TimeIncr\nKruskal-Wallis chi-squared          826 149.7574 1.670623e-26       ok  Monthly\n                            Season\nKruskal-Wallis chi-squared Monthly\n\n#The Kruskal-Wallis test has a p value of 1.670623e-26 which means the data is highly seasonal.\n\nSeason_Output[[3]]\n\n\n\n\n\n\n\n#This figure shows it nicely - it looks like nitrate concentrations peak over the winter months and are their lowest in summer. \n\n#We need to run a seasonal trend analysis.\n\n\n\n\nDefine a co-variate and test relationship.\n\nOkay, we know the data is seasonal, but could this be caused by seasonality in a co-variate, e.g. river discharge or rainfall? This is where co-variate adjustment comes in handy. The LWPTrends package allows us to determine the relationship between our observations and a potential co-variate, and then adjust our final values based on the values of the co-variate. For example, the concentration of an analyte is often related to discharge in rivers as this usually means there has been rainfall and mobilisation of contaminants sourced from the land. If this was the case, then an trends we find in our data could actually be caused by changes in river discharge rather than increased losses or loading from within the catchment.\nWith that being said, Snelder et al. (2021) do not recommend using flow adjusted trends for regional applications (i.e., assessing and reporting trends across many sites and variables in the context of regional state of environment reporting). They state that the purpose of flow-adjustment is to remove the confounding effect of flow so that the pattern of interest (the relationship between the observed water quality observations and time i.e., the trend) can be more confidently inferred. However, the definition of models describing observations of instantaneous flow is subjective and therefore there are unquantified uncertainties that arise due to procedural choices around flow adjustment that are likely to be made by individual analysts. Furthermore, there is evidence that trends are often associated with changes in the relationship between concentration and flow through the trend’s time period (Snelder and Kerr, 2022). However, flow adjustment (based on defining a relationship between observations and instantaneous flow) assumes that the concentration - flow relationship is constant through time. Violations of this assumption will affect the robustness of flow adjustment.\nSo in short, it’s complicated and somewhat subjective as to whether you should actually adjust values by discharge or not. But for the sake of this lesson, we will continue with the adjustment process using river discharge as a co-variate. The function we will use is called ‘AdjustValues()’ and there is a long list of possible inputs that we can change. Luckily, most of these have default values so we don’t really need to worry about them. We do, however, need to provide the dataset (referred to a ‘x’ - NNN_Te_Teko), the column that contains the values (referred to as ‘ValuesToAdjust’ - RawValue), and the column that contains the covariate (referred to as ‘Covariate’ - Discharge).\n\n#take the seasonally appended dataset\nNNN_Te_Teko &lt;- Season_Output[[1]]\n\n\nCV_Output&lt;-AdjustValues(NNN_Te_Teko, method = c(\"Gam\", \"LogLog\", \"LOESS\"), ValuesToAdjust = \"RawValue\", Covariate = \"Discharge\", Span = c(0.7), do.plot =T, plotpval=T, mymain=\"Example 1a\")\n\nAdjustValues() produces yet another list object, but this time there are only two sub-list components: 1) a dataset of model performance (note that this is NOT an appended dataset), and 2) a plot of the different mondels.\nLet’s look at component 2 first.\n\nCV_Output[[2]]\n\n\n\n\n\n\n\n\nWe can see that all models are technically significant (p&lt;0.05). However, some models look better than others. The LogLog (green) model doesn’t appear to represents this releationship as well as the Gam or LOESS models. The Gam and LOESS models are close, but the LOESS model seems to provide better representation at lower flows. We can also see the R2 values in the plot legend, which align with our visual assessment, i.e., Gam and LOESS have an R2 of 0.23, i.e., discharge explains 23% of the variance in NNN concentrations.\nWe might decide that 23% is a significant amount, and that we need to address this. Let’s take the LOESS model, and we will also plot up the new dataset to see what our new flow adjusted points look like. As mentioned above, flow adjustment is not recommended for regional data, therefore LWP have not made the output from the AdjustValues function fit seamlessly into the next step. Therefore, we need to do some tidying up before we can move on. Let’s tidy up this output and plot the new adjusted values to see what the look like.\n\n#take the output from the AdjustValues function. \nCV_Adjusted_Values &lt;- CV_Output[[1]]\n\n#we will use the cbind (column bind) function to append on the relevant column from the CV adjusted output. \nNNN_Te_Teko &lt;- NNN_Te_Teko %&gt;% \n  cbind(CV_Adjusted_Values %&gt;% select(LOESS0.7)) %&gt;% \n  rename(\"Flow_Adjusted\" = \"LOESS0.7\")\n\n#create a time-series plot of the flow adjusted results.\nP_FA &lt;- NNN_Te_Teko %&gt;% \n  ggplot()+\n  geom_point(aes(x=Time,y=Flow_Adjusted))+\n  theme_bw()\n\n#create a time-series plot of the raw results.\nP_RD &lt;- NNN_Te_Teko %&gt;% \n  ggplot()+\n  geom_point(aes(x=Time,y=RawValue))+\n  theme_bw()\n\n#create a time-series plot of the discharge.\nP_DC &lt;- NNN_Te_Teko %&gt;% \n  ggplot()+\n  geom_line(aes(x=Time,y=Discharge),colour=\"midnightblue\")+\n  theme_bw()\n\n#combine them together in a ggarrange figure. \nggarrange(P_RD,P_FA,P_DC,nrow=3, align = \"h\")\n\n\n\n\n\n\n\n\nHmm..I will leave you to come to your own conclusion, but you can see why Snelder et al. (2021) think flow adjustment can be subjective.\nRegardless, let’s take the flow adjusted values through to the final two steps.\n\n\n\nRun trend analysis.\n\nWe know from step 4 that we need to run a seasonal trend analysis. The function for this is called ‘SeasonalTrendAnalysis()’ and we need to tell it to use our ‘Flow_Adjusted’ column, but keep the ‘RawValue’ column for descriptive statistics. You can do this using the code below:\n\nTrend_Analysis_Output_NNN_Te_Teko &lt;-SeasonalTrendAnalysis(NNN_Te_Teko, ValuesToUse=\"Flow_Adjusted\", RawValues=FALSE, ValuesToUseforMedian=\"RawValue\", mymain=\"Example 1a: Flow Adjusted Trend\", do.plot=T)\n\nTrend_Analysis_Output_NNN_Te_Teko[[2]]\n\n\n\n\n\n\n\n\n\n\n\nClassify the output.\n\nNow we can classify the output in the same way as before (using ‘AssignConfCat’).\n\nTrend_Analysis_Output_NNN_Te_Teko &lt;- Trend_Analysis_Output_NNN_Te_Teko[[1]]\nTrend_Analysis_Output_NNN_Te_Teko &lt;- Trend_Analysis_Output_NNN_Te_Teko %&gt;% mutate(analyte = \"NNN\")\n\nTrend_Analysis_Output_NNN_Te_Teko$Direction &lt;- AssignConfCat(Trend_Analysis_Output_NNN_Te_Teko,CatType=\"Improve\",Reverse=c(\"VC\"))\n\nTrend_Analysis_Output_NNN_Te_Teko\n\n  nObs nTimeIncr    S  VarS    D         tau         Z        p         C\n1  800       412 -290 57074 6910 -0.04196816 -1.209703 0.226393 0.8868035\n         Cd prop.censored prop.unique no.censorlevels TimeIncr SeasIncr Median\n1 0.8868035             0         0.5               0  Monthly  Monthly 0.3735\n  AnnualSenSlope      Sen_Lci      Sen_Uci AnalysisNote Percent.annual.change\n1  -0.0004142955 -0.001240764 2.973935e-05           ok            -0.1109225\n  TrendDirection analyte        Direction\n1     Decreasing     NNN Likely improving\n\n\nWhew. We finally have a result. You can see that NNN is ‘Likely improving’ between 1990 and 2025. Interesting, this is very different to the obvious visual trend in the raw data.\n\nChallenge 8: What would the result be if we took our un-adjusted values instead of our flow-adjusted values? Which version do you think is more appropriate?\n\n\nClick to see a solution\n\n\nTrend_Analysis_Output_NNN_Te_Teko_Raw &lt;-SeasonalTrendAnalysis(NNN_Te_Teko, ValuesToUse=\"RawValue\", ValuesToUseforMedian=\"RawValue\", mymain=\"Example 1a: Flow Adjusted Trend\", do.plot=T)\n\nTrend_Analysis_Output_NNN_Te_Teko_Raw[[2]]\n\n\n\n\n\n\n\n\n\nTrend_Analysis_Output_NNN_Te_Teko_Raw &lt;- Trend_Analysis_Output_NNN_Te_Teko_Raw[[1]]\nTrend_Analysis_Output_NNN_Te_Teko_Raw &lt;- Trend_Analysis_Output_NNN_Te_Teko_Raw %&gt;% mutate(analyte = \"NNN\")\n\nTrend_Analysis_Output_NNN_Te_Teko_Raw$Direction &lt;- AssignConfCat(Trend_Analysis_Output_NNN_Te_Teko_Raw,CatType=\"Improve\",Reverse=c(\"VC\"))\n\nTrend_Analysis_Output_NNN_Te_Teko_Raw\n\n  nObs nTimeIncr    S     VarS    D       tau        Z            p C\n1  826       420 2712 59795.33 7154 0.3790886 11.08654 1.458258e-28 1\n            Cd prop.censored prop.unique no.censorlevels TimeIncr SeasIncr\n1 7.291291e-29             0   0.3474576               0  Monthly  Monthly\n  Median AnnualSenSlope     Sen_Lci     Sen_Uci AnalysisNote\n1 0.3755    0.005538401 0.004753135 0.006391111           ok\n  Percent.annual.change TrendDirection analyte             Direction\n1               1.47494     Increasing     NNN Very likely degrading\n\n\nThe un-adjusted trend would be ‘Very likely degrading’.\n\n\nWell, that’s it for this lesson but we have only covered half of the trend analysis content. I will try to develop a follow up lesson where we use the rest of the WQ_Data dataset to look at batch processing."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html",
    "href": "workshops/R_Tutorial_4_2025.html",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "",
    "text": "This lesson is designed to provide you with experience in manipulating and plotting time series data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\npadr\nzoo\nggpubr\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\nlibrary(padr)\nlibrary(zoo)\nlibrary(ggpubr)\n\nToday we will be exploring high frequency data collected at the Rangitaiki at SH5 site, over the period of the 2023 Auckland Anniversary and Cyclone Gabrielle storm events. High frequency data can be extracted from the Aquarius database using the ‘getdata’ function, but to make things easier, these datasets have been pre-extracted and saved as a csv.\nLet’s load the discharge dataset to see what we are working with.\n\nDischarge_DF &lt;- read.csv(\"./data/Rangitaiki_Discharge.csv\")\n\nIt’s good practice to inspect a dataset after importing it. You can use the ‘str’ function, but the ‘summary’ function provides more information.\n\nChallenge 1: Use the ‘summary’ funtion to inspect Discharge_DF. What do you notice about the dataset? Can we plot Value vs. Time in it’s current state?\n\n\nClick to see a solution\n\n\nsummary(Discharge_DF)\n\n     Site           LocationName           Time               Value       \n Length:11521       Length:11521       Length:11521       Min.   : 4.202  \n Class :character   Class :character   Class :character   1st Qu.: 5.219  \n Mode  :character   Mode  :character   Mode  :character   Median : 5.745  \n                                                          Mean   : 6.076  \n                                                          3rd Qu.: 6.699  \n                                                          Max.   :12.776  \n    Quality      Approval         Qualifiers      Parameter        \n Min.   :600   Length:11521       Mode:logical   Length:11521      \n 1st Qu.:600   Class :character   NA's:11521     Class :character  \n Median :600   Mode  :character                  Mode  :character  \n Mean   :600                                                       \n 3rd Qu.:600                                                       \n Max.   :600                                                       \n     Unit          \n Length:11521      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n# We can see that 'Time' has been imported as a character.  We will need to\n# change this to a timestamp before attempting to plot the data, otherwise R\n# will treat 'Time' as a discrete value rather than a continuous timescale.\n\n\n\nLet’s convert ‘Time’ to a proper timestamp. To do this, we need to tell R where to find important information (i.e., the year, month, day, hour, second). Take a look at the time column in ‘Discharge_DF’. See that the format is generally ‘DAY/MONTH/YEAR HOUR:MINUTE’. We would write this in R as “%d/%m/%Y %H:%M”.\nThere are a few exceptions - notice that midnight values have been reduced to ‘DAY/MONTH/YEAR’ (i.e., “%d/%m/%Y”) - this is Excel’s way of ‘helping’ us, because why would we need to state HOURS and MINUTES if they both equal zero? Luckily there is a function called ‘parse_date_time’ that allows us to provide a backup format to revert to if the initial format doesn’t quite fit. Don’t forget to specify the tz=“etc/GMT+12” as we will be using multiple datasets and want to ensure the times are comparable.\n\nDischarge_DF &lt;- Discharge_DF %&gt;% \n  mutate(Time = parse_date_time(Time,orders = c(\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\"),tz=\"etc/GMT+12\"))\n\n#the order of format preference is determined by the order in the list provided to \"orders\".\n\nGreat! Notice that all timestamps are now in the format “YEAR-MONTH-DAY HOUR:MINUTE:SECONDS” (or “%Y-%m-%d %H:%M:%S”). This is R’s default way of storing timestamps, which aligns with the ISO 8601 standard for time and date representation. If your timestamps are NOT in this format then they are not recognised by R as a timestamp. Also be wary that just because your timestamps are in this format, doesn’t mean that R has recognised them as timestamps - they could have been imported as a ‘character’. It always pays to check.\nClear as mud? Right, let’s move on.\nWe can now plot the data to see what we are dealing with.\n\nDischarge_DF %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ #%b represents the month as a short word\n  xlab(NULL) #we know the x axis represents time.  I don't feel the need to show it.\n\n\n\n\n\n\n\n\nThis figure shows the discharge at ‘Rangitaiki at SH5’ between January 20th and March 1st 2023. The first discharge peak was caused by the Auckland Anniversary storm, the second was caused by Cyclone Gabrielle.\nNotice that the y-axis is not labelled correctly. It should say something like “Discharge (m^3/s)”.\n\nChallenge 2: Change the y axis label on this plot to ‘Discharge (m^3/s)’. Hint - ylab() changes the label on the y axis in ggplot. Bonus points if you can get R to recognise ^3 as a superscript.\n\n\nClick to see a solution\n\n\nDischarge_DF %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ \n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))\n\n\n\n\n\n\n\n#ylab('Discharge (m^3/s)') is also acceptable.  \n\n\n\nLet’s say we wanted to quickly work out the peak values from these separate events. One (very rough) method could be to split the dataset in two - values before the 8th of Feb are attributed to the Auckland Anniversary event, and those after are attributed to Cyclone Gabrielle.\n\nDischarge_DF %&gt;% \n  mutate(Event = ifelse(Time &lt;= as.POSIXct(\"2023-02-08\"),\"Auckland\",\"Gabrielle\")) %&gt;% \n  group_by(Event) %&gt;% \n  summarise(Max_Disc = max(Value,na.rm=T))\n\n# A tibble: 2 × 2\n  Event     Max_Disc\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Auckland     12.8 \n2 Gabrielle     7.58\n\n\nWe can see that the Auckland Anniversary event peaks at 12.8 m^3/s and the Cyclone Gabrielle event peaks at 7.58 m^3/s. For context, the maximum flow recorded at this site over the past 10 years is 14.01 m^3/s. We could extract the entire discharge record from this site to work out the flow percentile, or you could just trust me that the Auckland event is in the top 1% of flows recorded at this site, and the Gabrielle event is in the top 2%."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#overview",
    "href": "workshops/R_Tutorial_4_2025.html#overview",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "",
    "text": "This lesson is designed to provide you with experience in manipulating and plotting time series data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\npadr\nzoo\nggpubr\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\nlibrary(padr)\nlibrary(zoo)\nlibrary(ggpubr)\n\nToday we will be exploring high frequency data collected at the Rangitaiki at SH5 site, over the period of the 2023 Auckland Anniversary and Cyclone Gabrielle storm events. High frequency data can be extracted from the Aquarius database using the ‘getdata’ function, but to make things easier, these datasets have been pre-extracted and saved as a csv.\nLet’s load the discharge dataset to see what we are working with.\n\nDischarge_DF &lt;- read.csv(\"./data/Rangitaiki_Discharge.csv\")\n\nIt’s good practice to inspect a dataset after importing it. You can use the ‘str’ function, but the ‘summary’ function provides more information.\n\nChallenge 1: Use the ‘summary’ funtion to inspect Discharge_DF. What do you notice about the dataset? Can we plot Value vs. Time in it’s current state?\n\n\nClick to see a solution\n\n\nsummary(Discharge_DF)\n\n     Site           LocationName           Time               Value       \n Length:11521       Length:11521       Length:11521       Min.   : 4.202  \n Class :character   Class :character   Class :character   1st Qu.: 5.219  \n Mode  :character   Mode  :character   Mode  :character   Median : 5.745  \n                                                          Mean   : 6.076  \n                                                          3rd Qu.: 6.699  \n                                                          Max.   :12.776  \n    Quality      Approval         Qualifiers      Parameter        \n Min.   :600   Length:11521       Mode:logical   Length:11521      \n 1st Qu.:600   Class :character   NA's:11521     Class :character  \n Median :600   Mode  :character                  Mode  :character  \n Mean   :600                                                       \n 3rd Qu.:600                                                       \n Max.   :600                                                       \n     Unit          \n Length:11521      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n# We can see that 'Time' has been imported as a character.  We will need to\n# change this to a timestamp before attempting to plot the data, otherwise R\n# will treat 'Time' as a discrete value rather than a continuous timescale.\n\n\n\nLet’s convert ‘Time’ to a proper timestamp. To do this, we need to tell R where to find important information (i.e., the year, month, day, hour, second). Take a look at the time column in ‘Discharge_DF’. See that the format is generally ‘DAY/MONTH/YEAR HOUR:MINUTE’. We would write this in R as “%d/%m/%Y %H:%M”.\nThere are a few exceptions - notice that midnight values have been reduced to ‘DAY/MONTH/YEAR’ (i.e., “%d/%m/%Y”) - this is Excel’s way of ‘helping’ us, because why would we need to state HOURS and MINUTES if they both equal zero? Luckily there is a function called ‘parse_date_time’ that allows us to provide a backup format to revert to if the initial format doesn’t quite fit. Don’t forget to specify the tz=“etc/GMT+12” as we will be using multiple datasets and want to ensure the times are comparable.\n\nDischarge_DF &lt;- Discharge_DF %&gt;% \n  mutate(Time = parse_date_time(Time,orders = c(\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\"),tz=\"etc/GMT+12\"))\n\n#the order of format preference is determined by the order in the list provided to \"orders\".\n\nGreat! Notice that all timestamps are now in the format “YEAR-MONTH-DAY HOUR:MINUTE:SECONDS” (or “%Y-%m-%d %H:%M:%S”). This is R’s default way of storing timestamps, which aligns with the ISO 8601 standard for time and date representation. If your timestamps are NOT in this format then they are not recognised by R as a timestamp. Also be wary that just because your timestamps are in this format, doesn’t mean that R has recognised them as timestamps - they could have been imported as a ‘character’. It always pays to check.\nClear as mud? Right, let’s move on.\nWe can now plot the data to see what we are dealing with.\n\nDischarge_DF %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ #%b represents the month as a short word\n  xlab(NULL) #we know the x axis represents time.  I don't feel the need to show it.\n\n\n\n\n\n\n\n\nThis figure shows the discharge at ‘Rangitaiki at SH5’ between January 20th and March 1st 2023. The first discharge peak was caused by the Auckland Anniversary storm, the second was caused by Cyclone Gabrielle.\nNotice that the y-axis is not labelled correctly. It should say something like “Discharge (m^3/s)”.\n\nChallenge 2: Change the y axis label on this plot to ‘Discharge (m^3/s)’. Hint - ylab() changes the label on the y axis in ggplot. Bonus points if you can get R to recognise ^3 as a superscript.\n\n\nClick to see a solution\n\n\nDischarge_DF %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ \n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))\n\n\n\n\n\n\n\n#ylab('Discharge (m^3/s)') is also acceptable.  \n\n\n\nLet’s say we wanted to quickly work out the peak values from these separate events. One (very rough) method could be to split the dataset in two - values before the 8th of Feb are attributed to the Auckland Anniversary event, and those after are attributed to Cyclone Gabrielle.\n\nDischarge_DF %&gt;% \n  mutate(Event = ifelse(Time &lt;= as.POSIXct(\"2023-02-08\"),\"Auckland\",\"Gabrielle\")) %&gt;% \n  group_by(Event) %&gt;% \n  summarise(Max_Disc = max(Value,na.rm=T))\n\n# A tibble: 2 × 2\n  Event     Max_Disc\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Auckland     12.8 \n2 Gabrielle     7.58\n\n\nWe can see that the Auckland Anniversary event peaks at 12.8 m^3/s and the Cyclone Gabrielle event peaks at 7.58 m^3/s. For context, the maximum flow recorded at this site over the past 10 years is 14.01 m^3/s. We could extract the entire discharge record from this site to work out the flow percentile, or you could just trust me that the Auckland event is in the top 1% of flows recorded at this site, and the Gabrielle event is in the top 2%."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#data-gaps",
    "href": "workshops/R_Tutorial_4_2025.html#data-gaps",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "2 Data gaps",
    "text": "2 Data gaps\nContinuous data is always subject to data gaps, e.g., a sensor might fail and need to be fixed, or it could become fouled with algae and start producing unusable data.\nLet’s assume the discharge data between “2023-02-05” and “2023-02-12” is unusable and has been removed from the dataset. The code below removes this section from our dataset. Note the use of the ‘|’ operator which means ‘OR’ in R language, i.e., Time is less than “2023-02-05” OR greater than “2023-02-12”.\n\nDischarge_DF_Gaps &lt;- Discharge_DF %&gt;% \n  filter(Time &lt; as.POSIXct(\"2023-02-01\",tz=\"etc/GMT+12\")|Time &gt;= as.POSIXct(\"2023-02-12\",tz=\"etc/GMT+12\"))\n\nWe can plot this in the same way as above.\n\nDischarge_DF_Gaps %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ \n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))\n\n\n\n\n\n\n\n\nNotice that there is no gap in the figure. This is because the dataset continues from the 4th Feb to 13th Feb with no gaps to represent the missing data. Therefore, ggplot assumes that the 4th Feb is connected to the 13th Feb and directly connects the two points with a straight line. This is common when extracting continuous data from our aquarius database as the extraction process does not include gaps. Obviously this is not representative of the actual timeseries dataset, and can be misleading to people interpreting the plot.\nFortunately, there is a very handy package called ‘padr’. This package is made for timeseries data and will interpret the frequency of your data and ‘pad’ your dataset (insert gaps) where there is missing data. See what happens when we introduce the ‘pad()’ function to our tidyverse pipeline.\n\nDischarge_DF_Gaps &lt;- Discharge_DF_Gaps %&gt;% \n  pad()\n\nDischarge_DF_Gaps %&gt;% \n  ggplot(aes(x=Time,y=Value))+\n  geom_path()+\n  theme_bw()+\n  scale_x_datetime(date_breaks = \"10 days\", date_labels = \"%b-%d\")+ \n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))\n\n\n\n\n\n\n\n\nPadr has automatically detected a 5 minute interval, and ‘padded’ our dataset, ensuring there is a row for every timestep. This is much more representative of our dataset."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#interpolation",
    "href": "workshops/R_Tutorial_4_2025.html#interpolation",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "3 Interpolation",
    "text": "3 Interpolation\nIt may be appropriate to interpolate over gaps in certain instances. There are a couple of useful functions in the ‘zoo’ package that can help with this - ‘na.approx()’ and ‘na.spline()’. Both methods use pre and post gap information to estimate values across the gap. The first method ‘na.approx()’ uses a linear interpolation method, while ‘na.spline()’ uses a polynomial method. You can see what these look like below.\n\n#create new columns that interpolate na values using either method.\nDischarge_DF_Gaps &lt;- Discharge_DF_Gaps %&gt;% \n  mutate(Value_Spline = na.spline(Value)) %&gt;% \n  mutate(Value_Linear = na.approx(Value))\n\nDischarge_DF_Gaps %&gt;% \n  ggplot() +\n  geom_path(aes(x = Time, y = Value_Spline, color = \"Spline\")) +\n  geom_path(aes(x = Time, y = Value_Linear, color = \"Linear\")) +\n  geom_path(aes(x = Time, y = Value, color = \"Original\")) +\n  scale_color_manual(name = \"Legend\", \n                     values = c(\"Spline\" = \"blue\", \"Linear\" = \"red\", \"Original\" = \"black\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt looks like the spline method interpolates our data better in this case as the linear method is not realistic. Note that neither method is perfect - we lose the small bumps that are observed in the untouched record as they can’t be estimated from remnant data.\nWe will use the spline record from this point onwards."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#thickening",
    "href": "workshops/R_Tutorial_4_2025.html#thickening",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "4 Thickening",
    "text": "4 Thickening\nAnother great function in the ‘padr’ package is ‘thicken()’. This will create an additional column with timestamps at a reduced frequency. For example, you might thicken a 5 minute dataframe to an hourly dataframe, or even daily dataframe. You can plug this into the tidyverse pipeline to summarise the dataset at the new frequency.\nLet’s thicken our new spline interpolated dataset to hourly and daily intervals.\n\n#this is the dataset thickened to hourly intervals\nHourly_Disc &lt;- Discharge_DF_Gaps %&gt;% \n  select(Site, LocationName, Time, Value_Spline) %&gt;% \n  thicken(by = 'Time', interval = 'hour',colname = 'Hour_Stamp') %&gt;% \n  group_by(Hour_Stamp) %&gt;% \n  summarise(Value = mean(Value_Spline,na.rm=T)) \n\n \nhead(Hourly_Disc)\n\n# A tibble: 6 × 2\n  Hour_Stamp          Value\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2023-01-20 00:00:00  4.53\n2 2023-01-20 01:00:00  4.53\n3 2023-01-20 02:00:00  4.53\n4 2023-01-20 03:00:00  4.53\n5 2023-01-20 04:00:00  4.53\n6 2023-01-20 05:00:00  4.53\n\n\n\nChallenge 3: Thicken and summarise the ‘Discharge_DF_Gaps’ dataset to a daily timestamp. Assign this new dataset the name ‘Daily_Disc’\n\n\nClick to see a solution\n\n\n#this is the dataset thickened to daily intervals\nDaily_Disc &lt;- Discharge_DF_Gaps %&gt;% \n  select(Site, LocationName, Time, Value_Spline) %&gt;% \n  thicken(by = 'Time', interval = 'day', colname = 'Day_Stamp') %&gt;% \n  group_by(Day_Stamp) %&gt;% \n  summarise(Value = mean(Value_Spline,na.rm=T))\n\nhead(Daily_Disc)\n\n# A tibble: 6 × 2\n  Day_Stamp  Value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2023-01-20  4.51\n2 2023-01-21  4.45\n3 2023-01-22  4.40\n4 2023-01-23  4.35\n5 2023-01-24  4.31\n6 2023-01-25  4.28\n\n\n\n\nNotice that the ‘Day_Stamp’ is of the class ‘date’. This will make it difficult to compare to the ‘Hour_Stamp’ which is in the class ‘S3:POSIXct’. We will adress this further down.\nLet’s plot the hourly record. However, this time we will save the plot as a png file using ‘ggsave()’. Note that We need to assign the plot to an object (Hourly_Disc_Plot) to do this.\n\n#this is a plot object of hourly discharge\nHourly_Disc_Plot &lt;- \n  Hourly_Disc %&gt;% \n  ggplot()+\n  geom_path(aes(x=Hour_Stamp,y=Value))+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))+\n  ggtitle(\"Hourly Average\")+\n  scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\nggsave(plot = Hourly_Disc_Plot,filename = \"Hourly_Disc.png\",dpi=300)\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\n\nYou should now have a file in your project folder called ‘Hourly_Disc.png’.\n\nChallenge 4: Create a figure for the daily discharge object you created above and save it as ‘Daily_Disc.png’.\n\n\nClick to see a solution\n\n\nDaily_Disc_Plot &lt;- \n  Daily_Disc %&gt;% \n  mutate(Day_Stamp = as.POSIXct(Day_Stamp, tz=\"etc/GMT+12\")) %&gt;% \n  ggplot()+\n  geom_path(aes(x=Day_Stamp,y=Value))+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))+\n  ggtitle(\"Daily Average\")+\n  scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\nggsave(plot = Daily_Disc_Plot,filename = \"Daily_Disc.png\",dpi=300)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_path()`).\n\n\n\n\nThese figures are useful, but it would be better if they were arranged in one figure so we could directly compare them. This is where the package ‘ggpubr’ comes in handy. This package has a function called ‘ggarrange()’ which allows you to arrange discrete plot objects in a combined format - particularly useful for publications. We will add in the original 5min dataframe so we gain a good understanding of how thicken affects our data.\n\n#create a plot for \nOriginal_Plot &lt;- Discharge_DF_Gaps %&gt;% \n  ggplot()+\n  geom_path(aes(x=Time,y=Value_Spline))+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))+\n  ggtitle(\"Original 5min Data\")+\n  scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\nCombined_Plot &lt;- ggarrange(Original_Plot,Hourly_Disc_Plot, Daily_Disc_Plot, nrow=3, align=\"v\")\n\nWarning: Removed 300 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_path()`).\n\n\n\nChallenge 5: Save the combined ggarrange plot as a png with a dpi=300, height=6, and width = 8.\n\n\nClick to see a solution\n\n\nggsave(plot = Combined_Plot,filename = \"Disc_Comparison_Plot.png\",dpi=300,height=6,width=8)\n\n# Units are in inches as a default, but you can change them by specifying the units as either \"in\", \"cm\", \"mm\", \"px\"."
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#introducing-other-datasets",
    "href": "workshops/R_Tutorial_4_2025.html#introducing-other-datasets",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "5 Introducing other datasets",
    "text": "5 Introducing other datasets\nWe can also access other datasets that might provide more context for our discharge data. The most obvious is rainfall data. The nearest rainfall site to ‘Rangitaiki at SH5’ is a site called ‘Te Whaiti at Minginui’.\nRainfall data can be imported in the same way as discharge data. We also need to convert the time column to a timestamp.\n\nChallenge 6: Import the ‘TeWhaiti_Minginui_Rainfall.csv’ file located within the data folder. Once you have imported it, convert the Time column to a proper timestamp using ‘parse_date_time’.\n\n\nClick to see a solution\n\n\nRainfall_DF &lt;- read.csv(\"./data/TeWhaiti_Minginui_Rainfall.csv\")\n\nRainfall_DF &lt;- Rainfall_DF %&gt;% \n  mutate(Time = parse_date_time(Time,orders = c(\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\"),tz=\"etc/GMT+12\"))\n\n\n\nTake a look at the rainfall dataset. Notice that rainfall data seems to be recorded in on an hourly basis OR when the tipping bucket mechanism triggers (i,e., there is enough liquid in the bucket to cause it to tip). The data is recorded in increments of 0.48mm (which I assume is equivalent to the volume of one bucket).\nThis means that we are going to have to manipulate the dataset to gain any useful information. The obvious solution is to calculate the sum of rainfall that accumulates over a set period.\nWe can easily do this using a combination of ‘thicken()’ and ‘summarise()’.\nLet’s calculate and plot the total daily rainfall for this site. Note that thicken will create a column called ‘Time_day’ by default. This column will be in a date format because we are summarising per day.\n\nRainfall_DF %&gt;%\n  thicken(interval = \"day\")  %&gt;%\n  group_by(Time_day) %&gt;% \n  summarise(Daily_Rainfall = sum(Value,na.rm=T)) %&gt;% \n  mutate(Time_day = as.POSIXct(Time_day,tz=\"etc/GMT+12\")) %&gt;% \n  ggplot()+\n  geom_bar(stat=\"identity\",aes(x=Time_day,y=Daily_Rainfall),fill=\"midnightblue\")+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(\"Daily Rainfall (mm)\")+\n  xlim(as.POSIXct(\"2023-01-20\",tz=\"etc/GMT+12\"),as.POSIXct(\"2023-02-25\",tz=\"etc/GMT+12\"))\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nGreat, this seems to align with our previous discharge figures. However, it would be better if we could bring rainfall and discharge together and align their x axes. This type of figure is common in hydrology, and rainfall is often plotted as a second y axis with is reversed so that the bars come down from the top. However, R is really against using dual y axes. See this article if you want to read about it. You can force R to do this, but everything feels a bit ‘hacky’, i.e., you end up forcing ggplot to do things it doesn’t want to do.\nWe will abide by R’s plotting rules and use ‘ggarrange()’ to create a similar figure that is more widely accepted by data enthusiasts.\nFirst we need to modify the rainfall figure so that the rainfall comes down from the top of the figure. We will also change the format of ‘Time_day’, which is currently a date class, to a POSIXct class, so that it aligns with the higher frequency discharge axis. Finally, we will remove the x labels because we don’t need them; everything will be aligned with the discharge axis.\nWe will also recreate the discharge plot without a title.\n\nRainfall_Plot &lt;- Rainfall_DF %&gt;%\n  thicken(interval = \"day\")  %&gt;%\n  group_by(Time_day) %&gt;% \n  summarise(Daily_Rainfall = sum(Value,na.rm=T)) %&gt;% \n  mutate(Time_day = as.POSIXct(Time_day,tz=\"etc/GMT+12\")) %&gt;% \n  ggplot()+\n  geom_bar(stat=\"identity\",aes(x=Time_day,y=Daily_Rainfall),fill=\"midnightblue\")+\n  scale_y_reverse()+\n  theme_bw()+\n  xlab(NULL)+\n  theme(axis.text.x=element_blank())+\n  ylab(\"Daily Rainfall (mm)\")+\n    scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\nDischarge_Plot &lt;- Discharge_DF_Gaps %&gt;% \n  ggplot()+\n  geom_path(aes(x=Time,y=Value_Spline))+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(expression(paste(\"Discharge (\", m^3, \"/s)\")))+\n  scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\nNow we can use ggarrange to position this above the discharge figure. We will add one more argument to this function, called ‘heights’. This specifies the relative heights of the two panels. In this case we will make the top panel 0.6 (60%) of the height of the bottom panel.\n\nggarrange(Rainfall_Plot, Discharge_Plot,align=\"v\",nrow=2,heights = c(0.6,1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 300 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\n\n\n\n\n\n\n\n\n\nChallenge 7: This site also has a continuous nitrate sensor (thanks Erin and the EIS team). The data record is also stored in the data folder. See if you can import this data and add it to the figure above. Once you have done that, save the entire combined figure as a png file with dpi=300, height=6, and width=8. I suggest adding Nitrate as the middle panel, which means that you will need to use ‘theme(axis.text.x=element_blank())’ to remove x axis labels.\n\n\nClick to see a solution\n\n\nNitrate_DF &lt;- read.csv(\"./data/Rangitaiki_Nitrate.csv\")\n\nNitrate_DF &lt;- Nitrate_DF %&gt;% \n  mutate(Time = parse_date_time(Time,orders = c(\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\"),tz=\"etc/GMT+12\"))\n\nNitrate_Plot &lt;- Nitrate_DF %&gt;% \n  ggplot()+\n  geom_path(aes(x=Time,y=Value))+\n  theme_bw()+\n  xlab(NULL)+\n  ylab(\"Nitrate (mg/l)\")+\n  theme(axis.text.x=element_blank())+\n  scale_x_datetime(date_breaks = \"10 days\", \n                   date_labels = \"%b-%d\",\n                   limits = c(as.POSIXct(\"2023-01-20\"), as.POSIXct(\"2023-03-01\")))\n\n\nCombined_Plot &lt;- ggarrange(Rainfall_Plot, Nitrate_Plot, Discharge_Plot,align=\"v\",nrow=3,heights = c(0.6,1,1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 99 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\n\nWarning: Removed 300 rows containing missing values or values outside the scale range\n(`geom_path()`).\n\nggsave(plot = Combined_Plot, filename = \"Combined_Rain_Nitrate_Disc.png\",width=8,height=6,dpi=300)"
  },
  {
    "objectID": "workshops/R_Tutorial_4_2025.html#merging-data",
    "href": "workshops/R_Tutorial_4_2025.html#merging-data",
    "title": "BOPRC R Tutorial 4 - Manipulating and plotting time series data: Part 2 (high frequency data)",
    "section": "6 Merging Data",
    "text": "6 Merging Data\nIn this final example we will see how our nitrate sensor compares against discrete NNN measurements from lab samples. Discrete lab samples are collected as part of a routine monthly run or via autosampler as part of a storm event sampling programme (thanks again Erin and EIS). These samples will not line up exactly with our high-resolution dataset, so we can’t just look up the nitrate reading at that time.\nLet’s read in the discrete lab samples and convert ‘Time’ to a proper timestamp.\n\nLab_Samples &lt;- read.csv(\"./data/Rangitaiki_NNN.csv\")\n\nLab_Samples &lt;- Lab_Samples %&gt;% \n  mutate(Time = parse_date_time(Time,orders = c(\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\"),tz=\"etc/GMT+12\"))\n\nNotice that the timestamps for this dataset are generally at an inconsistent frequency, where the timestamps for the continuous nitrate are every 15 minutes.\nThe trick to merging these datasets is to develop a timestamp that matches both datasets, i.e., a unique variable that we can merge with.\nLet’s create a new column in the Lab_Samples dataframe where the timestamp is rounded to the nearest 15 minutes. We can use our friend ‘thicken()’ to do this.\n\nLab_Samples &lt;- Lab_Samples %&gt;% \n  thicken(interval = \"15 min\")\n\nhead(Lab_Samples %&gt;% select(Time, Time_15_min))\n\n                 Time         Time_15_min\n1 2023-01-26 14:52:00 2023-01-26 14:45:00\n2 2023-01-27 21:40:00 2023-01-27 21:30:00\n3 2023-01-27 21:47:00 2023-01-27 21:45:00\n4 2023-01-27 21:56:00 2023-01-27 21:45:00\n5 2023-02-02 08:43:00 2023-02-02 08:30:00\n6 2023-02-02 12:42:00 2023-02-02 12:30:00\n\n\nNow we can merge the two datasets by the new timestamp. We probably don’t want all of the extra columns in the Lab_Samples or Nitrate_DF datasets, so we will limit this to the bare minimum using ‘select()’.\nThe merging process will be carried out by the ‘merge()’ function, which is part of base R (you don’t need a package). We need to tell this function the name of the variable in each dataset that we are merging by. Merge will rename column headings ‘COLUMN.x’ or ‘COLUMN.y’ if the names are identical between the two datasets, which will result in ‘Value.x’ and ‘Value.y’ in the example below. Therefore, we have used the ‘rename()’ function to make these more intuitive.\n\nMerged_Nitrate_DF &lt;- merge(Lab_Samples %&gt;% \n      select(Time_15_min,Value),\n      Nitrate_DF %&gt;% select(Time,Value),\n      by.x=\"Time_15_min\",\n      by.y=\"Time\") %&gt;% \n  rename(\"Lab\"=Value.x, \"Sensor\" = Value.y)\n\nhead(Merged_Nitrate_DF)\n\n          Time_15_min    Lab Sensor\n1 2023-01-26 14:45:00 1.4591 1.6277\n2 2023-01-27 21:30:00 1.4566 1.6341\n3 2023-01-27 21:45:00 1.4615 1.6303\n4 2023-01-27 21:45:00 1.4739 1.6303\n5 2023-02-02 08:30:00 1.1458 1.3033\n6 2023-02-02 12:30:00 1.1082 1.3076\n\n\nFinally, let’s create a plot to see how these values compare to each other!\n\nChallenge 8: Create a simple geom_point plot that compares Lab readings and Sensor readings. Label the each axis with either ‘Sensor Nitrate (mg/l)’ or ‘Lab NNN (mg/l)’. Set the axis limits so they are the same using ‘xlim(1,1.75)’ and ‘ylim(1,1.75)’. Add a reference 1:1 line using ‘geom_abline(slope = 1,lty=2)’. Finally let’s add a linear model using ‘geom_smooth(method = ’lm’)’. What can you determine about the sensor vs lab relationship over this time period?\n\n\nClick to see a solution\n\n\nMerged_Nitrate_DF %&gt;% \n  ggplot(aes(x=Lab,y=Sensor))+\n  geom_point()+\n  theme_bw()+\n  ylab(\"Sensor Nitrate (mg/l)\")+\n  xlab(\"Lab NNN (mg/l)\")+\n  geom_abline(slope = 1,lty=2)+\n  xlim(1,1.75)+\n  ylim(1,1.75)+\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n#it looks like the sensor slightly over-predicts at low concentrations and under-predicts at high concentrations."
  },
  {
    "objectID": "workshops/R_Tutorial_2_2025.html",
    "href": "workshops/R_Tutorial_2_2025.html",
    "title": "Lesson 2: Introduction to BOPRC’s Interal Databases and Packages",
    "section": "",
    "text": "These packages are only used by the Bay of Plenty Regional Council (BOPRC) and are therefore not publicly available. Please see Lesson 2: Introduction to BOPRC’s Interal Databases and Packages for a vignette on this tutorial."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R Workshop Series",
    "section": "",
    "text": "This website hosts the materials for a 6-part Introduction to R Workshop series. These tutorials were developed in May-June 2025 as a set of resources for training users in manipulating, visualizing, and analyzing environmental data.\nYou can read through the tutorials on the website, but we recommend copying the code to your local RStudio, running through the script, and completing the challenges without looking at the answers!\nThe original workshop materials, including data to run the code, are stored in individual repositories at the following links:\n\nLesson 1: Fundamentals of R\nLesson 2: Introduction to BOPRC’s Interal Databases and Packages\nLesson 3: Manipulating and plotting time series data in R: Part 1 (monthly data)\nLesson 4: Manipulating and plotting time series data in R: Part 2 (high-frequency data)\nLesson 5: Statistical analyses in R\nLesson 6: Trend analysis\n\nClick here for an introductory presentation before completing the tutorials."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These tutorials were made by Whitney Woelmer and James Dare as a collaboration between the University of Waikato and the Bay of Plenty Regional Council. They are intended to provide an introductory understanding in using RStudio for manipulating, visualizing, and analyzing environmental data.\nThe tutorials are provided for free and available to use by any.\nPlease contact Whitney at wwoelmer[at]waikato.ac.nz or James at james.dare[at]boprc.govt.nz with any questions."
  },
  {
    "objectID": "workshops/R_Tutorial_1_2025.html",
    "href": "workshops/R_Tutorial_1_2025.html",
    "title": "Lesson 1 - Fundamentals of R",
    "section": "",
    "text": "This introductory lesson is designed to teach you the fundamentals of using RStudio to analyze environmental data. First we will learn how to install packages.\n\n\nPackages provide new functions that can help us do specific actions in R. These will become increasingly useful as you learn more about R!\nThe main packages that we will use in this tutorial are:\n\ntidyverse\nreadxl\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nYou can also install packages directly using code.\n\ninstall.packages('tidyverse') # typically, R needs things to be in quotes, \n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wwoelmer\\AppData\\Local\\Temp\\RtmpA1yRVJ\\downloaded_packages\n\n# either '' or \"\" if it is not a recognized object in the environment\ninstall.packages('readxl') \n\npackage 'readxl' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wwoelmer\\AppData\\Local\\Temp\\RtmpA1yRVJ\\downloaded_packages\n\n\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\n\nYou can check if you have loaded your package by clicking on the ‘Packages’ tab, and navigate to the package name you just loaded. There should be a check next to it if it has been loaded properly. If you don’t see the package at all, that means it has not been installed.\n\n\n\nOne of the main ways that you will interact with R is to create and re-use objects. Let’s just creating a few objects ourselves. We did this for an in-person workshop where we counted the number of participants and instructors. However, you can change the code whatever you like! If you’re alone, maybe try changing it to the number of doorknobs and windows in your room.\n\nnum_participants &lt;- 13# insert number of people in the room\n  \nnum_instructors &lt;- 3# insert number of instructors in the room\n  \nparticipant_instructor_ratio &lt;- num_participants/num_instructors\n\nVoila! You’ve created new objects! You should now see the objects in the upper right of your RStudio, in the Environment window.\n\n\n\nYou can read in many different file formats into R and each will use their own function (e.g., read.csv, read.table, read_excel). To read in a file, you need to tell R where the file is located, relative to your working directory. To check where R is looking for your files, we will run the following function:\n\ngetwd()\n\n[1] \"C:/Users/wwoelmer/OneDrive - The University of Waikato/Desktop/R_Workshop_Website/workshops\"\n\n\nRunning getwd() tells you where your working directory is located. Since we are using a project, your working directory will be where you put your project on your computer. If you don’t use a project, you will need to set a working directory using setwd(). However, I DO NOT recommend setting working directories for reproducibility reasons. If someone else wanted to run my code, they won’t have a C:/Users/wwoelmer/Desktop/uni_files/ folder and will have to re-write the code to their own local directory…this causes lots of headaches. Projects are the best way to organize your files. But see other resources about this if desired: https://rpubs.com/em_/wdInR\nNow let’s read in our water quality data\n\nwq &lt;- read.csv('./data/BoP_WQ_formatted.csv') # HINT: hit 'tab' as you're typin the directory to see a list of files in this directory\n\nThe ./ notation means: look in the working directory (that is what the period represents), then in the folder data, then look for a file called BoP_WQ_formatted.csv\nNow that we’ve read in our data, it’s best practice to look at it and see if everything looks alright.\n\nView(wq) # this opens up the dataframe to view, \n# you can also do this by clicking on your dataframe ('wq') in the Environment at right\n\n\n\n\nNow that we’ve read in our data, let’s look at its structure.\n\nstr(wq)\n\n'data.frame':   3673 obs. of  20 variables:\n $ lake            : chr  \"Okareka\" \"Okareka\" \"Okareka\" \"Okareka\" ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : chr  \"2000-07-11\" \"2000-08-23\" \"2000-09-28\" \"2000-10-25\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nRunning the str function tells us the structure of each column in the dataframe. Now that we’ve looked at the structure of the wq dataframe, this shows us that the first three columns are of the character (chr) class, including wq$date But R has a specific data class for dates so we need to tell R that this is a date\n\nwq$date &lt;- as.Date(wq$date)\n\nAnother formal class in R is a POSIXct object, which include date and time. Because datetimes are often in UTC, we will use as.POSIXct() to specify the time zone and ensure the right date\nIn this case, we will create a new column called ‘datetime’ which store both date and time zone ETC/GMT+12\n\nwq$datetime &lt;- as.POSIXct(wq$date, tz = \"ETC/GMT+12\")\n\nNow that we’ve set the date column as class date and created a new column of class POSIXct, let’s look at the structure again\n\nstr(wq)\n\n'data.frame':   3673 obs. of  21 variables:\n $ lake            : chr  \"Okareka\" \"Okareka\" \"Okareka\" \"Okareka\" ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : Date, format: \"2000-07-11\" \"2000-08-23\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n $ datetime        : POSIXct, format: \"2000-07-10 12:00:00\" \"2000-08-22 12:00:00\" ...\n\n\nAnother data class we may want to use is ‘factor’. In wq, lake is listed as a character which means the different values have no particular order, and will appear alphabetically.\nLet’s say we wanted to order them by increasing maximum depth.\n\nwq$lake &lt;- factor(wq$lake, levels = c('Rotoehu', 'Rerewhakaaitu', 'Okaro',\n                                      'Tikitapu', 'Rotokakahi', 'Okareka',\n                                      'Rotorua', 'Okataina', 'Rotoma',\n                                      'Tarawera', 'Rotoiti', 'Rotomahana'))\n\n\nChallenge 1: What is the structure of wq now that you have updated the lake column?\n\n\nClick to see a solution\n\n\nstr(wq)\n\n'data.frame':   3673 obs. of  21 variables:\n $ lake            : Factor w/ 12 levels \"Rotoehu\",\"Rerewhakaaitu\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : Date, format: \"2000-07-11\" \"2000-08-23\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n $ datetime        : POSIXct, format: \"2000-07-10 12:00:00\" \"2000-08-22 12:00:00\" ...\n\n# This shows that the lake column is now a factor!\n\n\n\n\n\n\nNow let’s plot the wq data using the ggplot system. ggplot requires a dataframe (here, wq), and then the aesthetics or aes(). This tells it what to put on the x-axis and the y-axis. We have also told it to color the points based on the column site.\n\nggplot(wq, aes(x = as.Date(date), y = DRP_mgm3_top, color = site)) + \n  geom_point() + # this tells R how to \"map\" the data: in this case, use points\n  facet_wrap(~lake, scales = 'free') + # this makes a different panel for each lake, where the scale of both axes are different for each lake\n  theme_bw() # this sets a 'theme' for how the plot looks, this is the 'black and white' setting\n\nWarning: Removed 322 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nChallenge 2: Create a different plot with chla_mgm3_top on the y-axis.\n\n\nClick to see a solution\n\n\nggplot(wq, aes(x = as.Date(date), y = chla_mgm3_top, color = site)) + \n  geom_point() + # this tells R how to \"map\" the data: in this case, use points\n  facet_wrap(~lake, scales = 'free') + # this makes a different panel for each lake, where the scale of both axes are different for each lake\n  theme_bw() # this sets a 'theme' for how the plot looks, this is the 'black and white' setting\n\nWarning: Removed 97 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "workshops/R_Tutorial_1_2025.html#overview",
    "href": "workshops/R_Tutorial_1_2025.html#overview",
    "title": "Lesson 1 - Fundamentals of R",
    "section": "",
    "text": "This introductory lesson is designed to teach you the fundamentals of using RStudio to analyze environmental data. First we will learn how to install packages.\n\n\nPackages provide new functions that can help us do specific actions in R. These will become increasingly useful as you learn more about R!\nThe main packages that we will use in this tutorial are:\n\ntidyverse\nreadxl\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nYou can also install packages directly using code.\n\ninstall.packages('tidyverse') # typically, R needs things to be in quotes, \n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wwoelmer\\AppData\\Local\\Temp\\RtmpA1yRVJ\\downloaded_packages\n\n# either '' or \"\" if it is not a recognized object in the environment\ninstall.packages('readxl') \n\npackage 'readxl' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wwoelmer\\AppData\\Local\\Temp\\RtmpA1yRVJ\\downloaded_packages\n\n\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\n\nYou can check if you have loaded your package by clicking on the ‘Packages’ tab, and navigate to the package name you just loaded. There should be a check next to it if it has been loaded properly. If you don’t see the package at all, that means it has not been installed.\n\n\n\nOne of the main ways that you will interact with R is to create and re-use objects. Let’s just creating a few objects ourselves. We did this for an in-person workshop where we counted the number of participants and instructors. However, you can change the code whatever you like! If you’re alone, maybe try changing it to the number of doorknobs and windows in your room.\n\nnum_participants &lt;- 13# insert number of people in the room\n  \nnum_instructors &lt;- 3# insert number of instructors in the room\n  \nparticipant_instructor_ratio &lt;- num_participants/num_instructors\n\nVoila! You’ve created new objects! You should now see the objects in the upper right of your RStudio, in the Environment window.\n\n\n\nYou can read in many different file formats into R and each will use their own function (e.g., read.csv, read.table, read_excel). To read in a file, you need to tell R where the file is located, relative to your working directory. To check where R is looking for your files, we will run the following function:\n\ngetwd()\n\n[1] \"C:/Users/wwoelmer/OneDrive - The University of Waikato/Desktop/R_Workshop_Website/workshops\"\n\n\nRunning getwd() tells you where your working directory is located. Since we are using a project, your working directory will be where you put your project on your computer. If you don’t use a project, you will need to set a working directory using setwd(). However, I DO NOT recommend setting working directories for reproducibility reasons. If someone else wanted to run my code, they won’t have a C:/Users/wwoelmer/Desktop/uni_files/ folder and will have to re-write the code to their own local directory…this causes lots of headaches. Projects are the best way to organize your files. But see other resources about this if desired: https://rpubs.com/em_/wdInR\nNow let’s read in our water quality data\n\nwq &lt;- read.csv('./data/BoP_WQ_formatted.csv') # HINT: hit 'tab' as you're typin the directory to see a list of files in this directory\n\nThe ./ notation means: look in the working directory (that is what the period represents), then in the folder data, then look for a file called BoP_WQ_formatted.csv\nNow that we’ve read in our data, it’s best practice to look at it and see if everything looks alright.\n\nView(wq) # this opens up the dataframe to view, \n# you can also do this by clicking on your dataframe ('wq') in the Environment at right\n\n\n\n\nNow that we’ve read in our data, let’s look at its structure.\n\nstr(wq)\n\n'data.frame':   3673 obs. of  20 variables:\n $ lake            : chr  \"Okareka\" \"Okareka\" \"Okareka\" \"Okareka\" ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : chr  \"2000-07-11\" \"2000-08-23\" \"2000-09-28\" \"2000-10-25\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nRunning the str function tells us the structure of each column in the dataframe. Now that we’ve looked at the structure of the wq dataframe, this shows us that the first three columns are of the character (chr) class, including wq$date But R has a specific data class for dates so we need to tell R that this is a date\n\nwq$date &lt;- as.Date(wq$date)\n\nAnother formal class in R is a POSIXct object, which include date and time. Because datetimes are often in UTC, we will use as.POSIXct() to specify the time zone and ensure the right date\nIn this case, we will create a new column called ‘datetime’ which store both date and time zone ETC/GMT+12\n\nwq$datetime &lt;- as.POSIXct(wq$date, tz = \"ETC/GMT+12\")\n\nNow that we’ve set the date column as class date and created a new column of class POSIXct, let’s look at the structure again\n\nstr(wq)\n\n'data.frame':   3673 obs. of  21 variables:\n $ lake            : chr  \"Okareka\" \"Okareka\" \"Okareka\" \"Okareka\" ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : Date, format: \"2000-07-11\" \"2000-08-23\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n $ datetime        : POSIXct, format: \"2000-07-10 12:00:00\" \"2000-08-22 12:00:00\" ...\n\n\nAnother data class we may want to use is ‘factor’. In wq, lake is listed as a character which means the different values have no particular order, and will appear alphabetically.\nLet’s say we wanted to order them by increasing maximum depth.\n\nwq$lake &lt;- factor(wq$lake, levels = c('Rotoehu', 'Rerewhakaaitu', 'Okaro',\n                                      'Tikitapu', 'Rotokakahi', 'Okareka',\n                                      'Rotorua', 'Okataina', 'Rotoma',\n                                      'Tarawera', 'Rotoiti', 'Rotomahana'))\n\n\nChallenge 1: What is the structure of wq now that you have updated the lake column?\n\n\nClick to see a solution\n\n\nstr(wq)\n\n'data.frame':   3673 obs. of  21 variables:\n $ lake            : Factor w/ 12 levels \"Rotoehu\",\"Rerewhakaaitu\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ site            : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ date            : Date, format: \"2000-07-11\" \"2000-08-23\" ...\n $ chla_mgm3_top   : num  5.9 7.7 3.7 0.9 1.9 ...\n $ DRP_mgm3_bottom : num  2 0.5 0.5 4 6 2.25 3.5 3 2.9 5.85 ...\n $ DRP_mgm3_top    : num  0.5 0.5 0.5 0.5 2.5 0.5 1 0.5 3.55 0.5 ...\n $ NH4_mgm3_bottom : num  5 6 NA 11 8 3 15.5 7 20 31 ...\n $ NH4_mgm3_top    : num  6 9 3.5 2 10 ...\n $ NNN_mgm3_bottom : num  1 0.5 3 0.5 63 ...\n $ NNN_mgm3_top    : num  0.5 1 0.5 47.5 0.5 ...\n $ secchi_m        : num  6.5 6.9 9.1 11.1 8.4 9.3 9.4 9.6 9.6 9.7 ...\n $ TN_mgm3_bottom  : num  208 184 205 198 293 ...\n $ TN_mgm3_top     : num  176 184 196 268 234 ...\n $ TP_mg_m3_bottom : num  7 8 5 NA 6 ...\n $ TP_mgm3_top     : num  11 6 8.5 4 4 ...\n $ turb_NTU_bottom : num  1.06 0.93 1.4 2.4 0.97 ...\n $ turb_NTU_top    : num  1.5 1.1 0.75 0.7 0.83 ...\n $ pH_bottom       : num  7.3 7.4 7.2 7.1 7.1 ...\n $ pH_top          : num  7.2 7.4 7.55 7.9 7.85 ...\n $ chla_mgm3_bottom: num  NA NA NA NA NA NA NA NA NA NA ...\n $ datetime        : POSIXct, format: \"2000-07-10 12:00:00\" \"2000-08-22 12:00:00\" ...\n\n# This shows that the lake column is now a factor!\n\n\n\n\n\n\nNow let’s plot the wq data using the ggplot system. ggplot requires a dataframe (here, wq), and then the aesthetics or aes(). This tells it what to put on the x-axis and the y-axis. We have also told it to color the points based on the column site.\n\nggplot(wq, aes(x = as.Date(date), y = DRP_mgm3_top, color = site)) + \n  geom_point() + # this tells R how to \"map\" the data: in this case, use points\n  facet_wrap(~lake, scales = 'free') + # this makes a different panel for each lake, where the scale of both axes are different for each lake\n  theme_bw() # this sets a 'theme' for how the plot looks, this is the 'black and white' setting\n\nWarning: Removed 322 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nChallenge 2: Create a different plot with chla_mgm3_top on the y-axis.\n\n\nClick to see a solution\n\n\nggplot(wq, aes(x = as.Date(date), y = chla_mgm3_top, color = site)) + \n  geom_point() + # this tells R how to \"map\" the data: in this case, use points\n  facet_wrap(~lake, scales = 'free') + # this makes a different panel for each lake, where the scale of both axes are different for each lake\n  theme_bw() # this sets a 'theme' for how the plot looks, this is the 'black and white' setting\n\nWarning: Removed 97 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "workshops/R_Tutorial_1_2025.html#subset-data-plot-data-calculate-summary-statistics-write-a-csv",
    "href": "workshops/R_Tutorial_1_2025.html#subset-data-plot-data-calculate-summary-statistics-write-a-csv",
    "title": "Lesson 1 - Fundamentals of R",
    "section": "2 Subset data, plot data, calculate summary statistics, write a csv",
    "text": "2 Subset data, plot data, calculate summary statistics, write a csv\nWe are now going to learn to subset data. Here, let’s subset the wq dataframe to select just one lake, I’ll pick Rotoehu. Below is an example of subsetting using the base functions in R\n\nrotoehu &lt;- wq[wq$lake=='Rotoehu',] # the == means: look for an exact match\n  # this uses the base R notation of subset via brackets and indexing [rows, columns]\n  # here, we are saying take the dataframe wq\n  # then in brackets, we subset. here, we are saying keep only the rows where \n  # column 'lake' equals 'Rotoehu'. then we have a comma, and nothing after it,\n  # which means keep all of the columns\n\nA more intuitive way to subset dataframe is to use the tidyverse function filter()\n\nrotoehu &lt;- wq %&gt;%  # this symbol is called a pipe, you can read it as 'whereby'\n  filter(lake=='Rotoehu') # here we say filter out every row where the lake column\n                          # equals 'Rotoehu' (remember R is sensitive to capitals)\n\nUsing our subsetted dataframe rotoehu, let’s plot the data using ggplot. We can also clean up our axis labels a bit using the functions xlab and ylab.\n\nggplot(rotoehu, aes(x = as.Date(date), y = chla_mgm3_top)) + \n  geom_point() +\n  theme_bw() +\n  xlab('Date') +\n  ylab('Chl-a (mg/m3)')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nChallenge 3: Modify the ggplot code above to plot TN_mgm3_top in the rotoehu dataframe. Make sure your axis labels are accurate.\n\n\nClick to see a solution\n\n\nggplot(rotoehu, aes(x = as.Date(date), y = TN_mgm3_top)) + geom_point() + theme_bw() +\n    xlab(\"Date\") + ylab(\"Total Nitrogen (mg/m3)\")\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 Writing a .csv file\nNow let’s save this subsetted data as a new csv file. First, we will bring up the help documentation for the function write.csv() so we can see what information (called arguments) the function needs us to input\n\n?write.csv # bring up the help documentation\n\nBased on the help documentation, we can see there are lots of arguments, but most of them have defaults. The information that R needs to know includes x, which is the object we are exporting (in this case, the dataframe rotoehu), file which corresponds to the the location where we want to save the file (in this case, we will save it in the data folder and called the file rotoehu_wq.csv), and we want to set the argument row.names = FALSE so that the file isn’t written with an extra column naming the rows\n\nwrite.csv(rotoehu, # this is the object we want to export\n          file = './data/rotoehu_wq.csv',  # the . means go from the working directory, \n          row.names = FALSE) # which is our project directory (check getwd() to clarify)\n                             # then we are writing inside the 'data' folder\n                             # and can call the file whatever we want, with the \n                             # .csv extension. here, I've named it 'rotoehu_wq_2000_2021.csv\n                             # the row.names should be set as FALSE\n                             # to avoid having an extra column in the csv file which lists the row number\n\n\n\n2.2 Calculating summary statistics\nNow that we’ve subset our data, let’s calculate some summary statistics and save them as a new object\n\nmean_chl &lt;- mean(rotoehu$chla_mgm3_top)\nprint(mean_chl) #### hm that says NA, which means we need to remove the NA's before we take the mean\n\n[1] NA\n\n\nHmmm that says the mean_chl is NA. Look at the rotoehu dataframe: are all the chla values NA? No…which means there must be some NA’s in there which have thrown R off. We need to remove the NA’s before we take the mean. Look at help documentation (?mean) and read about the na.rm argument. We need to add the argument na.rm, then rerun the mean calculation\n\nChallenge 4: Use the na.rm argument within the mean function to calculate the mean chl-a in Lake Rotoehu.\n\n\nClick to see a solution\n\n\nmean_chl &lt;- mean(rotoehu$chla_mgm3_top, na.rm = TRUE)\nprint(mean_chl)\n\n[1] 11.24727\n\n\n\n\n\nChallenge 5: You’re on a roll! Not calculate the standard deviation using the function sd(). Don’t forget to use the na.rm function.\n\n\nClick to see a solution\n\n\nsd_chl &lt;- sd(rotoehu$chla_mgm3_top, na.rm = TRUE)\nprint(sd_chl)\n\n[1] 9.716998\n\n\n\n\n\nChallenge 6: Lastly, calculate the minimum and maximum values of chl-a in Lake Rotoehu.\n\n\nClick to see a solution\n\n\nmin_chl &lt;- min(rotoehu$chla_mgm3_top, na.rm = TRUE)\nmax_chl &lt;- max(rotoehu$chla_mgm3_top, na.rm = TRUE)\nprint(min_chl)\n\n[1] 2.34\n\nprint(max_chl)\n\n[1] 96.4\n\n\n\n\n\nChallenge 7: To wrap up this section, let’s have a big challenge where you repeat this excercise but for Lake Rotoma DRP instead. Remember that you need to 1) subset the original wq dataframe using either indexing or filter(), 2) plot the surface (top) DRP data for Lake Rotoma, 3) calculate the mean, sd, minimum, and maximum of DRP.\n\n\nClick to see a solution\n\n\nrotoma &lt;- wq %&gt;%\n    filter(lake == \"Rotoma\")\n\nggplot(rotoma, aes(x = as.Date(date), y = DRP_mgm3_top)) + geom_point() + theme_bw() +\n    xlab(\"Date\") + ylab(\"Dissolved Reactive Phosporus at surface (mg/m3)\")\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nmean(rotoma$DRP_mgm3_top, na.rm = TRUE)\n\n[1] 1.517479\n\nsd(rotoma$DRP_mgm3_top, na.rm = TRUE)\n\n[1] 1.647017\n\nmin(rotoma$DRP_mgm3_top, na.rm = TRUE)\n\n[1] -1\n\nmax(rotoma$DRP_mgm3_top, na.rm = TRUE)\n\n[1] 10.1286"
  },
  {
    "objectID": "workshops/R_Tutorial_3_2025.html",
    "href": "workshops/R_Tutorial_3_2025.html",
    "title": "Lesson 3 - Manipulating and plotting time series data in R",
    "section": "",
    "text": "This lesson is designed to provide you with experience in manipulating and plotting time series data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\nlubridate\nBoPRC2025\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\nlibrary(lubridate)\nlibrary(BoPRC2025) \n\nFirst we will load in our data. This data has been downloaded from Aquarius using the R script which you can find at scripts/download_data_aquarius.R if you’d like to see how the data were downloaded. For today, we are skipping that step and reading in directly from a .csv file which was written after the Aquarius download.\n\nwq &lt;- read.csv('./data/Lake_WQ_Timeseries.csv')\n\nNow, look at the wq dataframe by clicking on it in the environment and familiarise yourself with the columns. There is a lot of metadata here, but let’s select just a few which we want to work with, using the select function\n\nwq &lt;- wq %&gt;% \n  select(LocationName:Value, Parameter, Unit) # list the columns you want to keep, you can use, e.g. col1:col3 to select a range of columns\n\nOne of the columns that we have selected is the Time column, which includes both a date and a time. It is always best practice is to format date/time objects with the appropriate timezone, otherwise R will assume a timezone, and that can lead to the wrong date being set for your timestamp. Here, we will use a function called parse_date_time which looks at the Time column, and then provides a list (using c()) of potential formats that the column will be in. Here, we list two formats, the first one has YMD and HMS (hours, minutes, seconds), the second one just has YMD, as some of the values in the Time column don’t have an associated time next to the date. We pair this with the mutate function, which we will learn more about below.\nNOTE: there are many ways to format/parse dates and times in R. This is just one example!\n\nwq &lt;- wq %&gt;% mutate(Time = parse_date_time(Time,c(\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d\"), tz = \"etc/GMT+12\"))\n\n\nChallenge 1: What is the structure of wq now that you have updated the Time column?\n\n\nClick to see a solution\n\n\nstr(wq)\n\n'data.frame':   6469 obs. of  5 variables:\n $ LocationName: chr  \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" ...\n $ Time        : POSIXct, format: \"2015-01-20 07:53:00\" \"2015-02-17 07:40:00\" ...\n $ Value       : num  0.117 0.115 0.111 0.123 0.124 ...\n $ Parameter   : chr  \"TN (g/m^3)\" \"TN (g/m^3)\" \"TN (g/m^3)\" \"TN (g/m^3)\" ...\n $ Unit        : chr  \"g/m^3\" \"g/m^3\" \"g/m^3\" \"g/m^3\" ...\n\n# This shows that the Time column is now a POSIXct object\n\n\n\nUsing the unique function, let’s see what lakes which are included in this dataset.\n\nunique(wq$LocationName)\n\n [1] \"Lake Rotoma at Site 1 (Integrated)\"       \n [2] \"Lake Rotoehu at Site 3 (Integrated)\"      \n [3] \"Lake Rotoiti at Site 4 (Integrated)\"      \n [4] \"Lake Rotoiti at Site 3 (Integrated)\"      \n [5] \"Lake Rotoiti at Okawa Bay (Integrated)\"   \n [6] \"Lake Rotorua at Site 2 (Integrated)\"      \n [7] \"Lake Okataina at Site 1 (Integrated)\"     \n [8] \"Lake Okareka at Site 1 (Integrated)\"      \n [9] \"Lake Tikitapu at Site 1 (Integrated)\"     \n[10] \"Lake Rerewhakaaitu at Site 1 (Integrated)\"\n[11] \"Lake Okaro at Site 1 (Integrated)\"        \n[12] \"Lake Rotorua at Site 5 (Integrated)\"      \n[13] \"Lake Tarawera at Site 5 (Integrated)\"     \n[14] \"Lake Rotomahana at Site 2 (Integrated)\"   \n\n\n\nChallenge 2: Using the same unique function, what water quality variables are included in this dataset, in the Parameter column?\n\n\nClick to see a solution\n\n\nunique(wq$Parameter)\n\n[1] \"TN (g/m^3)\"    \"TP (g/m^3)\"    \"CHLA (mg/m^3)\" \"VC - SD (m)\"  \n\n# This shows we have total nitrogen, total phosphorus, chlorophyll-a, and\n# Secchi depth\n\n\n\nThe names of the parameters have spaces and symbols in them which can be annoying to work with in R. Let’s clean that up. We will use a function called recode which can be used to change the name of a value in a column. Here, we are saying take the wq dataframe, and mutate the column Parameter such that the values of Parameter which currently equal “TN (g/m^3)”, will be rewritten as “TN_gm3”. We will also do this for TP here.\n\nwq &lt;- wq %&gt;%\n    mutate(Parameter = recode(Parameter, `TN (g/m^3)` = \"TN_gm3\"), Parameter = recode(Parameter,\n        `TP (g/m^3)` = \"TP_gm3\"))\n\nAs with anything in R, there are multiple ways to rename entries within a column like we have just done. We will rename the TN and TP values in the Parameter column using the case_when function so you can learn another method. Sometimes one method may be more intuitive to you than another.\n\nwq &lt;- wq %&gt;% \n  mutate(Parameter = case_when(  # create a new `Parameter` column based on a set of conditions\n    Parameter == \"TN (g/m^3)\" ~ \"TN_gm3\", # first condition: if the value of Parameter is \"TN (g/m^3)\", change it to \"TN_gm3\"\n    Parameter == \"TP (g/m^3)\" ~ \"TP_gm3\", # same but for TP\n    TRUE ~ Parameter)) # for any other cases, keep the original values of Parameter\n\n# provide case_when example\n\n\nChallenge 3: We have done this for TN and TP. Now try using the same method to rename chlorophyll-a and Secchi depth using either recode or case_when. Make sure to name the new columns chla_mgm3 and secchi_m (we will use these same names later in the code so they will need to match!)\n\n\nClick to see a solution\n\n\nwq &lt;- wq %&gt;%\n    mutate(Parameter = recode(Parameter, `CHLA (mg/m^3)` = \"chla_mgm3\"), Parameter = recode(Parameter,\n        `VC - SD (m)` = \"secchi_m\"))\n\n\n\nNow let’s make a plot of our data, using the facet_wrap function to display the different parameters\n\nggplot(wq, aes(x = as.Date(Time), y = Value, color = LocationName)) + geom_point() +\n    facet_wrap(~Parameter, scales = \"free\") + theme_bw()\n\n\n\n\n\n\n\n# the scales = 'free' in the facet_wrap() function allows the x and y-axes to\n# vary by each panel\n\n\n\nLet’s say we’re interested in a plot of chl-a, but we want to color it based on the Secchi depth in that lake. Our dataset is in long format, so in order to do this, we need to make it into wide format (e.g., instead of Parameters as a column, TN, TP, chla, and Secchi will be their own columns, with the values in that column). We will use the pivot_wider function to do this, where you give the function the name of the column where the new columns will come from (here names_from = 'Parameter'), and the name of the column where the actual numbers will come from (here values_from = 'Value')\n\nwq_wide &lt;- wq %&gt;%\n    pivot_wider(names_from = \"Parameter\", values_from = \"Value\")\n\nWarning: Values from `Value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(LocationName, Time, Unit,\n  Parameter)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nThat threw some warnings, so let’s look at the dataframe and see if everything looks ok. When we open up wq_wide, we see there are a lot of “NULL” values for the different columns–that doesn’t look right. That is because our Time column is very specific, and includes not just dates but times which are not common across all the parameters. The time components isn’t really important in this case. We also have the Unit column, which is not the same across parameters and is causing an issue. Let’s create a Date column, remove Time and Unit, and try making the dataframe wide again\n\nwq_wide &lt;- wq %&gt;%\n    mutate(Date = as.Date(Time)) %&gt;%\n    select(-Time, -Unit) %&gt;%\n    pivot_wider(names_from = \"Parameter\", values_from = \"Value\")\n\nWarning: Values from `Value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(LocationName, Date, Parameter)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nHmm, that still throws a warning. If we look at wq_wide again, we see that there are some columns which have two values in them (e.g., Lake Rotoma on 2015-01-20 has two entries for TP). To fix this, let’s take the average on a given day for a given lake if there is more than one reading. We will need to introduce a couple of new functions to accomplish this.\n\n\n\ngroup_by is a tidyverse function which allows you to do calculations by different groups (e.g., the LocationName column). It is usually paired with another function which does the calculation. For example, summarise is another tidyverse function which, as it sounds, summarises by a given function. I often use this to take the mean, minimum, or some other summary statistic. This function results in a shorter dataframe, because you’ve summarised the values by your grouping factors. We will pair group_by with summarise to create summary statistics for each lake.\n\nwq_wide &lt;- wq %&gt;% \n  mutate(Date = as.Date(Time)) %&gt;% # keeping these lines in our workflow to remove Time and Unit\n  select(-Time, -Unit) %&gt;% \n  group_by(LocationName, Date, Parameter) %&gt;% \n  summarise(Value = mean(Value, na.rm = TRUE)) %&gt;% \n  pivot_wider(names_from = 'Parameter', values_from = 'Value') # then we pivot wider\n\nViola! No warnings and our dataframe looks good (make sure you look at it)!! Ok, now let’s make a plot of chl-a over time, but colored in by Secchi (our original goal before all that data manipulation…)\n\nwq_wide %&gt;% \n  filter(!is.na(secchi_m)) %&gt;% # some values have NA for secchi so we will remove those\n  ggplot(aes(x = as.Date(Date), y = chla_mgm3, color = secchi_m)) +\n  geom_point() +\n  facet_wrap(~LocationName) +\n  scale_color_distiller(palette = 'YlGnBu', direction = 2) +\n  theme_bw() +\n  scale_y_log10() # this log-transforms the y axis and makes it easier to see the variability across sites\n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# scale_color_distiller changes the color scheme,  you can google scale_color_distiller to find out other palettes you can use, direction = 2 just changes whether the scales goes from blue to yellow with blue as high or low, in this case, we want blue to be high. try changing direction = 1 and see what happens\n\n\nChallenge 4a: Now that we’ve used pivot_wider, try to use pivot_longer to turn your dataframe back into longer format\n\n\nClick to see a solution\n\n\nwq_long &lt;- wq_wide %&gt;%\n    pivot_longer(TN_gm3:secchi_m, names_to = \"Parameter\", values_to = \"Value\")\n\n# first you specify which columns are getting pivot-ted longer, we can use the\n# colon : to say all the columns between TN and secchi\n\n# being able to pivot between wide and long format is really helpful for\n# different types of analyses and plotting!\n\n\n\n\nChallenge 4b: Now that you’ve made your wq_long dataframe, try making a plot with Date on the x-axis, Value on the y-axis, color by LocationName, and facet_wrap by Parameter. Use geom_line instead of geom_point\n\n\nClick to see a solution\n\n\nggplot(wq_long, aes(x = as.Date(Date), y = Value, color = LocationName)) + geom_line() +\n    facet_wrap(~Parameter, scales = \"free\") + theme_bw() + scale_y_log10()\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve now plotted the full time series but we want to get a more summarised overview of the water quality across the lakes, so we will calculate some summary statistics. Let’s calculate the annual means for each variable and lake. First we need to create a new column which represents the year. We will use the function year in the lubridate package\n\nwq_summary &lt;- wq %&gt;%\n    mutate(year = year(Time)) %&gt;%\n    group_by(LocationName, Parameter, year) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE))\n\nOften, we actually need to calculate annual averages by the hydroyear, instead of the year. To do this, let’s use the Bathing_Season function in the BoPRC2025 package, which calculates the hydroyear.\n\nwq_summary &lt;- wq %&gt;%\n    # mutate(year = year(Time)) %&gt;%\nmutate(hydroyear = Bathing_Season(Time)) %&gt;%\n    group_by(LocationName, Parameter, hydroyear) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE))\n\nView the wq_summary dataframe and familiarise yourself with it. Let’s plot the chl-a data to visualize it a bit more clearly.\n\nwq_summary %&gt;% \n  filter(Parameter=='chla_mgm3') %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point() +\n  facet_wrap(~LocationName, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1))  +\n  ylab('Mean Chl-a (mgm/m3)')\n\n\n\n\n\n\n\n\n\nChallenge 5: That’s a lot of lakes to wrap your head around. For this challenge, create a plot for just Lake Rotorua at Site 2, and facet by Parameter\n\n\nClick to see a solution\n\n\nwq_summary %&gt;% \n  filter(LocationName==\"Lake Rotorua at Site 2 (Integrated)\") %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point(size = 2) +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) # this rotates the x-axis label since the LocationNames are long\n\n\n\n\n\n\n\n\n\n\nWe can also add the standard deviation as error bars using geom_errorbar.\n\nwq_summary %&gt;% \n  filter(LocationName==\"Lake Rotorua at Site 2 (Integrated)\") %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd, width = 0.2)) +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) # this rotates the x-axis label since the LocationNames are long\n\n\n\n\n\n\n\n\nHowever, we know there is strong seasonal variability. Let’s look at seasonal means instead of overall\n\n\n\nThe mutate function creates an entirely new column in our dataframe. This works similarly to summarise which we used above, except instead of summarising into a smaller dataframe, we use mutate across all our data and maintain the same number of rows. In order to do seasonal means, we need to first create a season identifier. We will pair mutate with case_when which you used earlier. This function chains together multiple ifelse statements. Essentially, it allows you to perform a multiple conditional operations which says if(this condition), then do X, if not, do Y. Here, we are saying if the month of our time column is 12, 1, 2, make the new column ‘season’ = ‘summer’, and so on for the other month combination. The last argument TRUE ~ NA_character_ says, if none of the conditions are met, assign it as NA.\n\n# go back to our original wq dataframe, not the summarised one and add a new\n# column: season\nwq &lt;- wq %&gt;%\n    mutate(season = case_when(month(Time) %in% c(12, 1, 2) ~ \"summer\", month(Time) %in%\n        c(3, 4, 5) ~ \"autumn\", month(Time) %in% c(6, 7, 8) ~ \"winter\", month(Time) %in%\n        c(9, 10, 11) ~ \"spring\", TRUE ~ NA_character_))  # default case, if needed\n\nLet’s plot the range of values across each season\n\nggplot(wq, aes(x = season, y = Value)) + geom_boxplot() + facet_wrap(~Parameter,\n    scales = \"free\") + theme_bw() + scale_y_log10()\n\n\n\n\n\n\n\n\nChallenge 6: It’s nice to see the boxplots, but we want the actual numbers for median, min, max, etc.. Calculate summary statistics for each lake, season, and parameter. HINT: We will bring back our friends group_by and summarise for this.\n\n\nClick to see a solution\n\n\nwq_season_summary &lt;- wq %&gt;%\n    group_by(LocationName, season, Parameter) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE), perc_95 = quantile(Value, 0.95))\n\n# Open up your dataframe and see if all looks good. There should be one value\n# for each season, lake, and parameter\n\n\n\n\n\n\nWe have a dataframe with the ranges of the NPSFM bands for each variable. We will need to read this in, merge together with our wq_summary dataframe, which has the annual median values for each variable. Then we will plot the data. But first we will have to do a little data manipulating…\n\n# read in bands\nbands &lt;- read.csv(\"./data/NPSFM_bands.csv\")\n# we want to join `bands` with the `wq` dataframe, but the column name for\n# `Parameter` is called `variable`. We will rename this so it matches with `wq`\n\nbands &lt;- bands %&gt;%\n    rename(Parameter = variable)\n\nThe values of TN and TP in bands$Parameter are in mg/m3, whereas in wq_summary they are in g/m3. We need to first convert TN and TP in wq_summary into mg/m3 before we can merge the two dataframes together.\n\nwq_summary &lt;- wq_summary %&gt;% \n  select(LocationName, Parameter, hydroyear, median) %&gt;% # select the columns we need\n  mutate(median = ifelse(Parameter %in% c('TN_gm3', 'TP_gm3'), median*1000, median),\n         Parameter = recode(Parameter, \n                            'TN_gm3' = 'TN_mgm3',\n                            'TP_gm3' = 'TP_mgm3'))\nprint(unique(wq_summary$Parameter))\n\n[1] \"TN_mgm3\"   \"TP_mgm3\"   \"chla_mgm3\" \"secchi_m\" \n\n\nNow that we have the columns in both of the dataframes wq_summary and bands set up properly, we can combine the two dataframes using the function left_join. This is another tidyverse function that merges two dataframes based on one or more common columns. It will keep all the rows from the first (left) dataframe and add any matching information in the second (right) dataframe. Here the matching columns is Parameter\n\nnof &lt;- left_join(wq_summary, bands, by = \"Parameter\")\n\nWarning in left_join(wq_summary, bands, by = \"Parameter\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 5 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nThis gives us a warning that there are “many-to-many relationships between x and y.” Sometimes this warning means that there is something wrong with your dataframe merge, but in this case, this is actually ok. It is telling us that in bands there are multiple matches for a single value of Parameter in wq_summary. That is because there are four values of bands$band (a, b, c, and d) which correspond to a single row in wq_summary. This has resulted in our new dataframe nof being much longer than the original wq_summary but that is expected behavior so we can ignore this warning.\nNow that we have our matching dataframe, we need to create maximum and minimum x values for which the color shading will be mapped onto our plot. These simply need to correspond to maximum and minimum years of our dataframe\n\nnof &lt;- nof %&gt;%\n    group_by(Parameter) %&gt;%\n    mutate(x_max = max(hydroyear), x_min = min(hydroyear))\n\nIf we look at nof again, we should have a minimum and maximum column for both x and y. We will use these in our plotting by adding a layer using geom_rect to map the colors of the different bands behind our points\n\nggplot(nof, aes(x = hydroyear, y = median, color = as.factor(LocationName))) + geom_rect(aes(xmin = x_min,\n    xmax = x_max, ymin = y_min, ymax = y_max, fill = band, alpha = 0.2), color = NA) +\n    geom_point(size = 2) + scale_fill_manual(values = c(\"#00A2E1FF\", \"#62BD19FF\",\n    \"#FFC726FF\", \"#FF671FFF\")) + facet_wrap(~Parameter, scales = \"free\") + labs(color = \"Site\") +\n    theme_bw() + theme(axis.text.x = element_text(angle = 75, hjust = 1)) + scale_y_log10()\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 153 rows containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n\n\n\nChallenge 7: Secchi depth is not currently assessed as part of the NPSFM. Let’s remove it from our plot using the filter function:\n\n\nClick to see a solution\n\n\nnof %&gt;%\n    filter(Parameter != \"secchi_m\") %&gt;%\n    ggplot(aes(x = hydroyear, y = median, color = as.factor(LocationName))) + geom_rect(aes(xmin = x_min,\n    xmax = x_max, ymin = y_min, ymax = y_max, fill = band), color = NA) + geom_point(size = 2) +\n    scale_fill_manual(values = c(\"#00A2E1FF\", \"#62BD19FF\", \"#FFC726FF\", \"#FF671FFF\")) +\n    facet_wrap(~Parameter, scales = \"free\", nrow = 2) + labs(color = \"Site\") + theme_bw() +\n    theme(axis.text.x = element_text(angle = 75, hjust = 1)) + scale_y_log10()\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values."
  },
  {
    "objectID": "workshops/R_Tutorial_3_2025.html#overview",
    "href": "workshops/R_Tutorial_3_2025.html#overview",
    "title": "Lesson 3 - Manipulating and plotting time series data in R",
    "section": "",
    "text": "This lesson is designed to provide you with experience in manipulating and plotting time series data.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\nlubridate\nBoPRC2025\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nOnce all of these packages are installed you can load them using the ‘library’ function:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\nlibrary(lubridate)\nlibrary(BoPRC2025) \n\nFirst we will load in our data. This data has been downloaded from Aquarius using the R script which you can find at scripts/download_data_aquarius.R if you’d like to see how the data were downloaded. For today, we are skipping that step and reading in directly from a .csv file which was written after the Aquarius download.\n\nwq &lt;- read.csv('./data/Lake_WQ_Timeseries.csv')\n\nNow, look at the wq dataframe by clicking on it in the environment and familiarise yourself with the columns. There is a lot of metadata here, but let’s select just a few which we want to work with, using the select function\n\nwq &lt;- wq %&gt;% \n  select(LocationName:Value, Parameter, Unit) # list the columns you want to keep, you can use, e.g. col1:col3 to select a range of columns\n\nOne of the columns that we have selected is the Time column, which includes both a date and a time. It is always best practice is to format date/time objects with the appropriate timezone, otherwise R will assume a timezone, and that can lead to the wrong date being set for your timestamp. Here, we will use a function called parse_date_time which looks at the Time column, and then provides a list (using c()) of potential formats that the column will be in. Here, we list two formats, the first one has YMD and HMS (hours, minutes, seconds), the second one just has YMD, as some of the values in the Time column don’t have an associated time next to the date. We pair this with the mutate function, which we will learn more about below.\nNOTE: there are many ways to format/parse dates and times in R. This is just one example!\n\nwq &lt;- wq %&gt;% mutate(Time = parse_date_time(Time,c(\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d\"), tz = \"etc/GMT+12\"))\n\n\nChallenge 1: What is the structure of wq now that you have updated the Time column?\n\n\nClick to see a solution\n\n\nstr(wq)\n\n'data.frame':   6469 obs. of  5 variables:\n $ LocationName: chr  \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" \"Lake Rotoma at Site 1 (Integrated)\" ...\n $ Time        : POSIXct, format: \"2015-01-20 07:53:00\" \"2015-02-17 07:40:00\" ...\n $ Value       : num  0.117 0.115 0.111 0.123 0.124 ...\n $ Parameter   : chr  \"TN (g/m^3)\" \"TN (g/m^3)\" \"TN (g/m^3)\" \"TN (g/m^3)\" ...\n $ Unit        : chr  \"g/m^3\" \"g/m^3\" \"g/m^3\" \"g/m^3\" ...\n\n# This shows that the Time column is now a POSIXct object\n\n\n\nUsing the unique function, let’s see what lakes which are included in this dataset.\n\nunique(wq$LocationName)\n\n [1] \"Lake Rotoma at Site 1 (Integrated)\"       \n [2] \"Lake Rotoehu at Site 3 (Integrated)\"      \n [3] \"Lake Rotoiti at Site 4 (Integrated)\"      \n [4] \"Lake Rotoiti at Site 3 (Integrated)\"      \n [5] \"Lake Rotoiti at Okawa Bay (Integrated)\"   \n [6] \"Lake Rotorua at Site 2 (Integrated)\"      \n [7] \"Lake Okataina at Site 1 (Integrated)\"     \n [8] \"Lake Okareka at Site 1 (Integrated)\"      \n [9] \"Lake Tikitapu at Site 1 (Integrated)\"     \n[10] \"Lake Rerewhakaaitu at Site 1 (Integrated)\"\n[11] \"Lake Okaro at Site 1 (Integrated)\"        \n[12] \"Lake Rotorua at Site 5 (Integrated)\"      \n[13] \"Lake Tarawera at Site 5 (Integrated)\"     \n[14] \"Lake Rotomahana at Site 2 (Integrated)\"   \n\n\n\nChallenge 2: Using the same unique function, what water quality variables are included in this dataset, in the Parameter column?\n\n\nClick to see a solution\n\n\nunique(wq$Parameter)\n\n[1] \"TN (g/m^3)\"    \"TP (g/m^3)\"    \"CHLA (mg/m^3)\" \"VC - SD (m)\"  \n\n# This shows we have total nitrogen, total phosphorus, chlorophyll-a, and\n# Secchi depth\n\n\n\nThe names of the parameters have spaces and symbols in them which can be annoying to work with in R. Let’s clean that up. We will use a function called recode which can be used to change the name of a value in a column. Here, we are saying take the wq dataframe, and mutate the column Parameter such that the values of Parameter which currently equal “TN (g/m^3)”, will be rewritten as “TN_gm3”. We will also do this for TP here.\n\nwq &lt;- wq %&gt;%\n    mutate(Parameter = recode(Parameter, `TN (g/m^3)` = \"TN_gm3\"), Parameter = recode(Parameter,\n        `TP (g/m^3)` = \"TP_gm3\"))\n\nAs with anything in R, there are multiple ways to rename entries within a column like we have just done. We will rename the TN and TP values in the Parameter column using the case_when function so you can learn another method. Sometimes one method may be more intuitive to you than another.\n\nwq &lt;- wq %&gt;% \n  mutate(Parameter = case_when(  # create a new `Parameter` column based on a set of conditions\n    Parameter == \"TN (g/m^3)\" ~ \"TN_gm3\", # first condition: if the value of Parameter is \"TN (g/m^3)\", change it to \"TN_gm3\"\n    Parameter == \"TP (g/m^3)\" ~ \"TP_gm3\", # same but for TP\n    TRUE ~ Parameter)) # for any other cases, keep the original values of Parameter\n\n# provide case_when example\n\n\nChallenge 3: We have done this for TN and TP. Now try using the same method to rename chlorophyll-a and Secchi depth using either recode or case_when. Make sure to name the new columns chla_mgm3 and secchi_m (we will use these same names later in the code so they will need to match!)\n\n\nClick to see a solution\n\n\nwq &lt;- wq %&gt;%\n    mutate(Parameter = recode(Parameter, `CHLA (mg/m^3)` = \"chla_mgm3\"), Parameter = recode(Parameter,\n        `VC - SD (m)` = \"secchi_m\"))\n\n\n\nNow let’s make a plot of our data, using the facet_wrap function to display the different parameters\n\nggplot(wq, aes(x = as.Date(Time), y = Value, color = LocationName)) + geom_point() +\n    facet_wrap(~Parameter, scales = \"free\") + theme_bw()\n\n\n\n\n\n\n\n# the scales = 'free' in the facet_wrap() function allows the x and y-axes to\n# vary by each panel\n\n\n\nLet’s say we’re interested in a plot of chl-a, but we want to color it based on the Secchi depth in that lake. Our dataset is in long format, so in order to do this, we need to make it into wide format (e.g., instead of Parameters as a column, TN, TP, chla, and Secchi will be their own columns, with the values in that column). We will use the pivot_wider function to do this, where you give the function the name of the column where the new columns will come from (here names_from = 'Parameter'), and the name of the column where the actual numbers will come from (here values_from = 'Value')\n\nwq_wide &lt;- wq %&gt;%\n    pivot_wider(names_from = \"Parameter\", values_from = \"Value\")\n\nWarning: Values from `Value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(LocationName, Time, Unit,\n  Parameter)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nThat threw some warnings, so let’s look at the dataframe and see if everything looks ok. When we open up wq_wide, we see there are a lot of “NULL” values for the different columns–that doesn’t look right. That is because our Time column is very specific, and includes not just dates but times which are not common across all the parameters. The time components isn’t really important in this case. We also have the Unit column, which is not the same across parameters and is causing an issue. Let’s create a Date column, remove Time and Unit, and try making the dataframe wide again\n\nwq_wide &lt;- wq %&gt;%\n    mutate(Date = as.Date(Time)) %&gt;%\n    select(-Time, -Unit) %&gt;%\n    pivot_wider(names_from = \"Parameter\", values_from = \"Value\")\n\nWarning: Values from `Value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(LocationName, Date, Parameter)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nHmm, that still throws a warning. If we look at wq_wide again, we see that there are some columns which have two values in them (e.g., Lake Rotoma on 2015-01-20 has two entries for TP). To fix this, let’s take the average on a given day for a given lake if there is more than one reading. We will need to introduce a couple of new functions to accomplish this.\n\n\n\ngroup_by is a tidyverse function which allows you to do calculations by different groups (e.g., the LocationName column). It is usually paired with another function which does the calculation. For example, summarise is another tidyverse function which, as it sounds, summarises by a given function. I often use this to take the mean, minimum, or some other summary statistic. This function results in a shorter dataframe, because you’ve summarised the values by your grouping factors. We will pair group_by with summarise to create summary statistics for each lake.\n\nwq_wide &lt;- wq %&gt;% \n  mutate(Date = as.Date(Time)) %&gt;% # keeping these lines in our workflow to remove Time and Unit\n  select(-Time, -Unit) %&gt;% \n  group_by(LocationName, Date, Parameter) %&gt;% \n  summarise(Value = mean(Value, na.rm = TRUE)) %&gt;% \n  pivot_wider(names_from = 'Parameter', values_from = 'Value') # then we pivot wider\n\nViola! No warnings and our dataframe looks good (make sure you look at it)!! Ok, now let’s make a plot of chl-a over time, but colored in by Secchi (our original goal before all that data manipulation…)\n\nwq_wide %&gt;% \n  filter(!is.na(secchi_m)) %&gt;% # some values have NA for secchi so we will remove those\n  ggplot(aes(x = as.Date(Date), y = chla_mgm3, color = secchi_m)) +\n  geom_point() +\n  facet_wrap(~LocationName) +\n  scale_color_distiller(palette = 'YlGnBu', direction = 2) +\n  theme_bw() +\n  scale_y_log10() # this log-transforms the y axis and makes it easier to see the variability across sites\n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# scale_color_distiller changes the color scheme,  you can google scale_color_distiller to find out other palettes you can use, direction = 2 just changes whether the scales goes from blue to yellow with blue as high or low, in this case, we want blue to be high. try changing direction = 1 and see what happens\n\n\nChallenge 4a: Now that we’ve used pivot_wider, try to use pivot_longer to turn your dataframe back into longer format\n\n\nClick to see a solution\n\n\nwq_long &lt;- wq_wide %&gt;%\n    pivot_longer(TN_gm3:secchi_m, names_to = \"Parameter\", values_to = \"Value\")\n\n# first you specify which columns are getting pivot-ted longer, we can use the\n# colon : to say all the columns between TN and secchi\n\n# being able to pivot between wide and long format is really helpful for\n# different types of analyses and plotting!\n\n\n\n\nChallenge 4b: Now that you’ve made your wq_long dataframe, try making a plot with Date on the x-axis, Value on the y-axis, color by LocationName, and facet_wrap by Parameter. Use geom_line instead of geom_point\n\n\nClick to see a solution\n\n\nggplot(wq_long, aes(x = as.Date(Date), y = Value, color = LocationName)) + geom_line() +\n    facet_wrap(~Parameter, scales = \"free\") + theme_bw() + scale_y_log10()\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve now plotted the full time series but we want to get a more summarised overview of the water quality across the lakes, so we will calculate some summary statistics. Let’s calculate the annual means for each variable and lake. First we need to create a new column which represents the year. We will use the function year in the lubridate package\n\nwq_summary &lt;- wq %&gt;%\n    mutate(year = year(Time)) %&gt;%\n    group_by(LocationName, Parameter, year) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE))\n\nOften, we actually need to calculate annual averages by the hydroyear, instead of the year. To do this, let’s use the Bathing_Season function in the BoPRC2025 package, which calculates the hydroyear.\n\nwq_summary &lt;- wq %&gt;%\n    # mutate(year = year(Time)) %&gt;%\nmutate(hydroyear = Bathing_Season(Time)) %&gt;%\n    group_by(LocationName, Parameter, hydroyear) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE))\n\nView the wq_summary dataframe and familiarise yourself with it. Let’s plot the chl-a data to visualize it a bit more clearly.\n\nwq_summary %&gt;% \n  filter(Parameter=='chla_mgm3') %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point() +\n  facet_wrap(~LocationName, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1))  +\n  ylab('Mean Chl-a (mgm/m3)')\n\n\n\n\n\n\n\n\n\nChallenge 5: That’s a lot of lakes to wrap your head around. For this challenge, create a plot for just Lake Rotorua at Site 2, and facet by Parameter\n\n\nClick to see a solution\n\n\nwq_summary %&gt;% \n  filter(LocationName==\"Lake Rotorua at Site 2 (Integrated)\") %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point(size = 2) +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) # this rotates the x-axis label since the LocationNames are long\n\n\n\n\n\n\n\n\n\n\nWe can also add the standard deviation as error bars using geom_errorbar.\n\nwq_summary %&gt;% \n  filter(LocationName==\"Lake Rotorua at Site 2 (Integrated)\") %&gt;% # only plot TN\n  ggplot(aes(x = hydroyear, y = mean)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd, width = 0.2)) +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) # this rotates the x-axis label since the LocationNames are long\n\n\n\n\n\n\n\n\nHowever, we know there is strong seasonal variability. Let’s look at seasonal means instead of overall\n\n\n\nThe mutate function creates an entirely new column in our dataframe. This works similarly to summarise which we used above, except instead of summarising into a smaller dataframe, we use mutate across all our data and maintain the same number of rows. In order to do seasonal means, we need to first create a season identifier. We will pair mutate with case_when which you used earlier. This function chains together multiple ifelse statements. Essentially, it allows you to perform a multiple conditional operations which says if(this condition), then do X, if not, do Y. Here, we are saying if the month of our time column is 12, 1, 2, make the new column ‘season’ = ‘summer’, and so on for the other month combination. The last argument TRUE ~ NA_character_ says, if none of the conditions are met, assign it as NA.\n\n# go back to our original wq dataframe, not the summarised one and add a new\n# column: season\nwq &lt;- wq %&gt;%\n    mutate(season = case_when(month(Time) %in% c(12, 1, 2) ~ \"summer\", month(Time) %in%\n        c(3, 4, 5) ~ \"autumn\", month(Time) %in% c(6, 7, 8) ~ \"winter\", month(Time) %in%\n        c(9, 10, 11) ~ \"spring\", TRUE ~ NA_character_))  # default case, if needed\n\nLet’s plot the range of values across each season\n\nggplot(wq, aes(x = season, y = Value)) + geom_boxplot() + facet_wrap(~Parameter,\n    scales = \"free\") + theme_bw() + scale_y_log10()\n\n\n\n\n\n\n\n\nChallenge 6: It’s nice to see the boxplots, but we want the actual numbers for median, min, max, etc.. Calculate summary statistics for each lake, season, and parameter. HINT: We will bring back our friends group_by and summarise for this.\n\n\nClick to see a solution\n\n\nwq_season_summary &lt;- wq %&gt;%\n    group_by(LocationName, season, Parameter) %&gt;%\n    summarise(mean = mean(Value, na.rm = TRUE), median = median(Value, na.rm = TRUE),\n        min = min(Value, na.rm = TRUE), max = max(Value, na.rm = TRUE), sd = sd(Value,\n            na.rm = TRUE), perc_95 = quantile(Value, 0.95))\n\n# Open up your dataframe and see if all looks good. There should be one value\n# for each season, lake, and parameter\n\n\n\n\n\n\nWe have a dataframe with the ranges of the NPSFM bands for each variable. We will need to read this in, merge together with our wq_summary dataframe, which has the annual median values for each variable. Then we will plot the data. But first we will have to do a little data manipulating…\n\n# read in bands\nbands &lt;- read.csv(\"./data/NPSFM_bands.csv\")\n# we want to join `bands` with the `wq` dataframe, but the column name for\n# `Parameter` is called `variable`. We will rename this so it matches with `wq`\n\nbands &lt;- bands %&gt;%\n    rename(Parameter = variable)\n\nThe values of TN and TP in bands$Parameter are in mg/m3, whereas in wq_summary they are in g/m3. We need to first convert TN and TP in wq_summary into mg/m3 before we can merge the two dataframes together.\n\nwq_summary &lt;- wq_summary %&gt;% \n  select(LocationName, Parameter, hydroyear, median) %&gt;% # select the columns we need\n  mutate(median = ifelse(Parameter %in% c('TN_gm3', 'TP_gm3'), median*1000, median),\n         Parameter = recode(Parameter, \n                            'TN_gm3' = 'TN_mgm3',\n                            'TP_gm3' = 'TP_mgm3'))\nprint(unique(wq_summary$Parameter))\n\n[1] \"TN_mgm3\"   \"TP_mgm3\"   \"chla_mgm3\" \"secchi_m\" \n\n\nNow that we have the columns in both of the dataframes wq_summary and bands set up properly, we can combine the two dataframes using the function left_join. This is another tidyverse function that merges two dataframes based on one or more common columns. It will keep all the rows from the first (left) dataframe and add any matching information in the second (right) dataframe. Here the matching columns is Parameter\n\nnof &lt;- left_join(wq_summary, bands, by = \"Parameter\")\n\nWarning in left_join(wq_summary, bands, by = \"Parameter\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 5 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nThis gives us a warning that there are “many-to-many relationships between x and y.” Sometimes this warning means that there is something wrong with your dataframe merge, but in this case, this is actually ok. It is telling us that in bands there are multiple matches for a single value of Parameter in wq_summary. That is because there are four values of bands$band (a, b, c, and d) which correspond to a single row in wq_summary. This has resulted in our new dataframe nof being much longer than the original wq_summary but that is expected behavior so we can ignore this warning.\nNow that we have our matching dataframe, we need to create maximum and minimum x values for which the color shading will be mapped onto our plot. These simply need to correspond to maximum and minimum years of our dataframe\n\nnof &lt;- nof %&gt;%\n    group_by(Parameter) %&gt;%\n    mutate(x_max = max(hydroyear), x_min = min(hydroyear))\n\nIf we look at nof again, we should have a minimum and maximum column for both x and y. We will use these in our plotting by adding a layer using geom_rect to map the colors of the different bands behind our points\n\nggplot(nof, aes(x = hydroyear, y = median, color = as.factor(LocationName))) + geom_rect(aes(xmin = x_min,\n    xmax = x_max, ymin = y_min, ymax = y_max, fill = band, alpha = 0.2), color = NA) +\n    geom_point(size = 2) + scale_fill_manual(values = c(\"#00A2E1FF\", \"#62BD19FF\",\n    \"#FFC726FF\", \"#FF671FFF\")) + facet_wrap(~Parameter, scales = \"free\") + labs(color = \"Site\") +\n    theme_bw() + theme(axis.text.x = element_text(angle = 75, hjust = 1)) + scale_y_log10()\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 153 rows containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n\n\n\nChallenge 7: Secchi depth is not currently assessed as part of the NPSFM. Let’s remove it from our plot using the filter function:\n\n\nClick to see a solution\n\n\nnof %&gt;%\n    filter(Parameter != \"secchi_m\") %&gt;%\n    ggplot(aes(x = hydroyear, y = median, color = as.factor(LocationName))) + geom_rect(aes(xmin = x_min,\n    xmax = x_max, ymin = y_min, ymax = y_max, fill = band), color = NA) + geom_point(size = 2) +\n    scale_fill_manual(values = c(\"#00A2E1FF\", \"#62BD19FF\", \"#FFC726FF\", \"#FF671FFF\")) +\n    facet_wrap(~Parameter, scales = \"free\", nrow = 2) + labs(color = \"Site\") + theme_bw() +\n    theme(axis.text.x = element_text(angle = 75, hjust = 1)) + scale_y_log10()\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values."
  },
  {
    "objectID": "workshops/R_Tutorial_5_2025.html",
    "href": "workshops/R_Tutorial_5_2025.html",
    "title": "BOPRC R Tutorial 5 - Statistical Analyses in R",
    "section": "",
    "text": "This lesson is designed to provide you with experience in running statistical analyses in R. We will use water quality data as an example, but these analyses can be applied to many other datasets, provided the statistical assumptions are met. We will cover the following topics:\n\nCorrelation analyses (Pearson and Spearman) and plots\nLinear regression and plots\nT-tests and Wilcoxon rank sum test (aka Mann-Whitney)\nANOVA\n\n\n\n\n\n\n\nNote\n\n\n\nDisclaimer\nThis lesson teaches the implementation of multiple statistical analyses, rather than the background behind why, when, and what to check when choosing a statistical analysis. You should always check the underlying assumptions of an analysis and whether your data meet those assumptions.\n\n\nWe are adding a few new packages today which perform specialized functions for statistical analyses. You don’t need to worry about the new packages too much, other than you will need to install and load the libraries.\nThe main packages that we will use in this tutorial are:\n\ntidyverse\nlubridate\nHmisc\ncorrplot\nggpmisc\nggpubr\n\nBefore attempting to install these packages, make sure your Primary CRAN Repository is set to:\n\n“New Zealand [https] - University of Auckland”\n\nTo check this, click ‘Tools’ –&gt; ‘Global Options’ –&gt; ‘Packages’. Click ‘Change’ if you need to adjust this.\nYou can download most packages by clicking on the ‘Install’ button on the ‘packages’ tab in the lower right window pane. Then in the Install Packages popup, select ‘Repository (CRAN)’ from the ‘Install from’ drop box and type the name of the package you wish to download (e.g., dplyr).\nOnce all of these packages are installed you can load them using the library function:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\nlibrary(lubridate)\nlibrary(Hmisc)\nlibrary(corrplot)\nlibrary(ggpmisc)\n\nWarning: package 'ggpmisc' was built under R version 4.4.3\n\n\nWarning: package 'ggpp' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nFirst we will load in our data. We will use the same water quality data from Lesson 3. This data has been downloaded from Aquarius using the R script which you can find at scripts/download_data_aquarius.R if you’d like to see how the data were downloaded. For today, we are skipping that step and reading in directly from a .csv file which was written after the Aquarius download.\n\nwq &lt;- read.csv('./data/Lake_WQ_Timeseries.csv')\n\nNow, look at the wq dataframe by clicking on it in the environment and familiarise yourself with the columns. You can also run the function colnames(wq) in the console to get a list of column names. It’s better to run this in the console (rather than in your script editor), since it is a diagnostic test and not something you will necessary need to run every time you open your script–just as needed.\n\ncolnames(wq) # you don't have to save this in your script, but can copy it into the console\n\n [1] \"Site\"                \"LocationName\"        \"Time\"               \n [4] \"Value\"               \"Quality\"             \"Approval\"           \n [7] \"Qualifiers\"          \"Parameter\"           \"Unit\"               \n[10] \"Sample_Number\"       \"Project_Id\"          \"LowerDetectionLimit\"\n[13] \"UpperDetectionLimit\" \"Method\"              \"CollectionMethod\"   \n[16] \"Lab\"                 \"Error\"               \"Comment\"            \n[19] \"ACTION\"              \"Samp_Comment\"        \"Cloud_Cover\"        \n[22] \"Num_Swimmers\"        \"Tide_Cycle\"          \"Tide\"               \n[25] \"Wind_Direction\"      \"Wind_Speed\"          \"Weather_Today\"      \n[28] \"Weather_Yest\"        \"Surface_Cond\"        \"Bore_Type\"          \n[31] \"Bore_Number\"         \"Bore_Collect_Method\" \"Bore_Sampled_From\"  \n[34] \"Bore_Pump_Rate\"      \"Bore_Pump_Duration\"  \"Bore_Protocol\"      \n[37] \"Bore_Probe\"          \"Odour\"              \n\n\nAs we know from previous lessons, it is always best practice is to format date/time objects with the appropriate timezone, otherwise R will assume a timezone, and that can lead to the wrong date being set for your timestamp. This is the first thing I do when I see I have a datetime object as a column. Let’s use a bit of code that will parse our Time column, which includes both a date and a time.\nHere, we will use a function called parse_date_time which looks at the Time column, and then provides a list (using c()) of potential formats that the column will be in. Here, we list two formats, the first one has YMD and HMS (hours, minutes, seconds), the second one just has YMD, as some of the values in the Time column don’t have an associated time next to the date. We pair this with the mutate function to re-write our Time column.\nNOTE: there are many ways to format/parse dates and times in R. This is just one example!\n\nwq &lt;- wq %&gt;% mutate(Time = parse_date_time(Time,c(\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d\"), tz = \"etc/GMT+12\"))\n\n\nChallenge 1: What locations and parameters are included in this dataset? Use the unique() function to find out.\n\n\nClick to see a solution\n\n\nunique(wq$Parameter)\n\n[1] \"TN (g/m^3)\"    \"TP (g/m^3)\"    \"CHLA (mg/m^3)\" \"VC - SD (m)\"  \n\nunique(wq$LocationName)\n\n [1] \"Lake Rotoma at Site 1 (Integrated)\"       \n [2] \"Lake Rotoehu at Site 3 (Integrated)\"      \n [3] \"Lake Rotoiti at Site 4 (Integrated)\"      \n [4] \"Lake Rotoiti at Site 3 (Integrated)\"      \n [5] \"Lake Rotoiti at Okawa Bay (Integrated)\"   \n [6] \"Lake Rotorua at Site 2 (Integrated)\"      \n [7] \"Lake Okataina at Site 1 (Integrated)\"     \n [8] \"Lake Okareka at Site 1 (Integrated)\"      \n [9] \"Lake Tikitapu at Site 1 (Integrated)\"     \n[10] \"Lake Rerewhakaaitu at Site 1 (Integrated)\"\n[11] \"Lake Okaro at Site 1 (Integrated)\"        \n[12] \"Lake Rotorua at Site 5 (Integrated)\"      \n[13] \"Lake Tarawera at Site 5 (Integrated)\"     \n[14] \"Lake Rotomahana at Site 2 (Integrated)\"   \n\n\n\n\nThat helps us get a better understanding of the dataset that we’re working with and is something I will do often while working in R to remind myself.\nSince we are going to do correlation analysis first, let’s focus on just one site and look at the relationships between variables in the Parameter column. We will filter the data to only select “Lake Okaro at Site 1 (Integrated)” and we will create a new dataframe named wq_okaro so we keep all the other lake data in the wq dataframe.\n\nwq_okaro &lt;- wq %&gt;% \n  filter(LocationName=='Lake Okaro at Site 1 (Integrated)')\n\nLet’s also clean up the dataframe and only select the columns which are useful to us right now\n\nwq_okaro &lt;- wq_okaro %&gt;% \n  select(Time, Value, Parameter, Unit)\n\nLet’s plot the data to make sure everything looks good. I like to do a geom_point plot, as well as a histogram, using geom_histogram\n\nggplot(wq_okaro, aes(x = as.POSIXct(Time), y = Value, color = Parameter)) +\n  geom_point() +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw() +\n  xlab('Time')\n\n\n\n\n\n\n\nggplot(wq_okaro, aes(x = Value, fill = Parameter)) +\n  geom_histogram() +\n  facet_wrap(~Parameter, scales = 'free') +\n  theme_bw()\n\n\n\n\n\n\n\n\nAlright, we have a time series of four variables. There are many things we can do to analyse this data. Let’s start with a correlation analysis."
  },
  {
    "objectID": "workshops/R_Tutorial_5_2025.html#t-test",
    "href": "workshops/R_Tutorial_5_2025.html#t-test",
    "title": "BOPRC R Tutorial 5 - Statistical Analyses in R",
    "section": "4.1 T-test",
    "text": "4.1 T-test\nLet’s also create a boxplot which will show the distributions of data at each site. We will need to pivot_longer again to show the boxplots with the sites on the x-axis, so we will do this in the tidyverse pipe style, without creating a new object.\n\nt.test(log(rotoiti_wide$OkawaBay_chla), log(rotoiti_wide$Site4_chla), paired = TRUE)\n\n\n    Paired t-test\n\ndata:  log(rotoiti_wide$OkawaBay_chla) and log(rotoiti_wide$Site4_chla)\nt = 8.5839, df = 65, p-value = 2.689e-12\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.6182031 0.9930897\nsample estimates:\nmean difference \n      0.8056464 \n\nrotoiti_wide %&gt;%\n    pivot_longer(OkawaBay_chla:Site4_chla, names_to = \"Site\", values_to = \"Chla\") %&gt;%\n    ggplot(aes(x = Site, y = log(Chla), fill = Site)) + geom_boxplot() + theme_bw() +\n    ylab(\"Log Chl-a\") + stat_compare_means(method = \"t.test\", label = \"p.format\")\n\nWarning: Removed 92 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 92 rows containing non-finite outside the scale range\n(`stat_compare_means()`).\n\n\n\n\n\n\n\n\n\nThe results of our t-test show that these two sites are significantly different from each other. This isn’t too surprising given that Okawa Bay is a much shallower, isolated bay on the western end of Lake Rotoiti, while Site 4 is located in the much deeper main basin. Visually inspecting the boxplots also supports this. Cool!"
  },
  {
    "objectID": "workshops/R_Tutorial_5_2025.html#wilcoxon-rank-sum",
    "href": "workshops/R_Tutorial_5_2025.html#wilcoxon-rank-sum",
    "title": "BOPRC R Tutorial 5 - Statistical Analyses in R",
    "section": "4.2 Wilcoxon rank sum",
    "text": "4.2 Wilcoxon rank sum\nNow, let’s say we didn’t want to log-transform our data. We can use non-parametric statistical tests to look for differences between non-normally distributed datasets. We will use the Wilcoxon rank sum test for this (sometimes called the Mann-Whitney test).\nLet’s run the test using the wilcoxon.test function, and also create our boxplot figure. We will use the argument paired = TRUE in our Wilcoxon test because these samples were taken at roughly the same time and are expected to be representative of similar conditions at both sites.\n\nwilcox.test(rotoiti_wide$OkawaBay_chla, rotoiti_wide$Site4_chla, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  rotoiti_wide$OkawaBay_chla and rotoiti_wide$Site4_chla\nV = 2046.5, p-value = 1.877e-09\nalternative hypothesis: true location shift is not equal to 0\n\nrotoiti_wide %&gt;%\n    pivot_longer(OkawaBay_chla:Site4_chla, names_to = \"Site\", values_to = \"Chla\") %&gt;%\n    ggplot(aes(x = Site, y = Chla, fill = Site)) + geom_boxplot() + theme_bw() +\n    stat_compare_means(method = \"wilcox.test\", label = \"p.format\")\n\nWarning: Removed 92 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 92 rows containing non-finite outside the scale range\n(`stat_compare_means()`).\n\n\n\n\n\n\n\n\n\nWith the Wilcoxon rank sum test on our raw data, we also show that there is a statistical difference between these two sites. Pretty cool to see that two locations within the same lake are significantly different from each!\n__\nChallenge 7: Run either a t-test or a Wilcoxon rank sum test to test if there is a significant difference between Lake Rotoiti at Site 4 and Lake Rotorua at Site 5. You can choose whichever water quality variable you’d like to look at. Remember, you will need to start with the wq dataframe, select the relevant columns, filter LocationName and Parameter. Then you will pivot_wider (don’t forget to make a Date column and remove Unit), rename your columns, and run your statistical test plus a plot! HINT: Samples are not collected on the same date between Rotorua and Rotoiti, so you will need to format your dates as Month-Year (e.g., Jan-2021). You can do this using this line of code within your tidyverse pipe: mutate(Date = format(as.Date(Time), format = '%b-%Y'))\n\n\nClick to see a solution\n\n\nrotoiti_rotorua &lt;- wq %&gt;%\n    select(Time, LocationName, Value, Parameter, Unit) %&gt;%\n    filter(LocationName %in% c(\"Lake Rotorua at Site 5 (Integrated)\", \"Lake Rotoiti at Site 4 (Integrated)\"),\n        Parameter == \"TP (g/m^3)\")\n\nrotoiti_rotorua_wide &lt;- rotoiti_rotorua %&gt;%\n    mutate(Date = format(as.Date(Time), format = \"%b-%Y\")) %&gt;%\n    select(-Time, -Unit) %&gt;%\n    group_by(Date, LocationName, Parameter) %&gt;%\n    summarise(Value = mean(Value, na.rm = TRUE)) %&gt;%\n    pivot_wider(names_from = \"LocationName\", values_from = \"Value\") %&gt;%\n    ungroup()\n\nrotoiti_rotorua_wide &lt;- rotoiti_rotorua_wide %&gt;%\n    select(-Parameter) %&gt;%\n    rename(Rotoiti_TP = \"Lake Rotoiti at Site 4 (Integrated)\", Rotorua_TP = \"Lake Rotorua at Site 5 (Integrated)\")\n\n## if running a t-test\nt.test(log(rotoiti_rotorua_wide$Rotoiti_TP), log(rotoiti_rotorua_wide$Rotorua_TP))\n\n\n    Welch Two Sample t-test\n\ndata:  log(rotoiti_rotorua_wide$Rotoiti_TP) and log(rotoiti_rotorua_wide$Rotorua_TP)\nt = 3.0868, df = 231.54, p-value = 0.00227\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.05506289 0.24940133\nsample estimates:\nmean of x mean of y \n-3.829669 -3.981901 \n\nrotoiti_rotorua_wide %&gt;%\n    pivot_longer(Rotoiti_TP:Rotorua_TP, names_to = \"Site\", values_to = \"TP\") %&gt;%\n    ggplot(aes(x = Site, y = log(TP), fill = Site)) + geom_boxplot() + theme_bw() +\n    stat_compare_means(method = \"wilcox.test\", label = \"p.format\")\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_compare_means()`).\n\n\n\n\n\n\n\n\n## if running a wilcoxon test\nwilcox.test(rotoiti_rotorua_wide$Rotoiti_TP, rotoiti_rotorua_wide$Rotorua_TP)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rotoiti_rotorua_wide$Rotoiti_TP and rotoiti_rotorua_wide$Rotorua_TP\nW = 8444.5, p-value = 0.003098\nalternative hypothesis: true location shift is not equal to 0\n\nrotoiti_rotorua_wide %&gt;%\n    pivot_longer(Rotoiti_TP:Rotorua_TP, names_to = \"Site\", values_to = \"TP\") %&gt;%\n    ggplot(aes(x = Site, y = TP, fill = Site)) + geom_boxplot() + theme_bw() + stat_compare_means(method = \"wilcox.test\",\n    label = \"p.format\")\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_compare_means()`)."
  },
  {
    "objectID": "workshops/R_Tutorial_5_2025.html#anova",
    "href": "workshops/R_Tutorial_5_2025.html#anova",
    "title": "BOPRC R Tutorial 5 - Statistical Analyses in R",
    "section": "4.3 ANOVA",
    "text": "4.3 ANOVA\nWhat if we have more than two variables we want to test for differences across? This is where the ANOVA, or ANalysis Of VAriance comes in. An ANOVA assumes that your response variable is normally distributed and you are comparing differences across categorical predictors.\nIn this example, we will use an ANOVA to test for differences in secchi depth across season in Lake Okaro.\nWe will go back to our wq_okaro dataset. First, we need to filter to just secchi depth data and clean a few things up. Then, we test to see if secchi depth is normally distributed, and if not, we will transform it!\n\nhead(wq_okaro)\n\n                 Time Value  Parameter  Unit\n1 2015-01-22 10:27:00 0.900 TN (g/m^3) g/m^3\n2 2015-02-19 10:40:00 0.370 TN (g/m^3) g/m^3\n3 2015-03-19 09:50:00 0.400 TN (g/m^3) g/m^3\n4 2015-04-16 11:25:00 0.335 TN (g/m^3) g/m^3\n5 2015-05-21 09:50:00 0.583 TN (g/m^3) g/m^3\n6 2015-06-23 11:00:00 0.920 TN (g/m^3) g/m^3\n\nsecchi_okaro &lt;- wq_okaro %&gt;%\n    filter(Parameter == \"VC - SD (m)\") %&gt;%\n    select(-Unit, -Parameter) %&gt;%\n    rename(secchi_m = Value)\n\nhist(secchi_okaro$secchi_m)\n\n\n\n\n\n\n\nshapiro.test(secchi_okaro$secchi_m)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secchi_okaro$secchi_m\nW = 0.94404, p-value = 0.0003429\n\n\nLooks like Secchi is not normally distributed, so we will create a new log-transformed column and run the shapiro test again to check.\n\nsecchi_okaro &lt;- secchi_okaro %&gt;%\n    mutate(log_secchi_m = log(secchi_m))\n\nhist(secchi_okaro$log_secchi_m)\n\n\n\n\n\n\n\nshapiro.test(secchi_okaro$log_secchi_m)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secchi_okaro$log_secchi_m\nW = 0.97802, p-value = 0.09311\n\n\nP-value is greater than 0.05, so that looks better. We will run our ANOVA on log_secchi_m.\nNext, we need to create out seasons column, which is the variable by which we want to test if there are differences in Secchi depth. We first create a month column, and then create the season column, which is based on the month. I’ll do this first for ‘Summer’.\n\nsecchi_okaro &lt;- secchi_okaro %&gt;%\n    mutate(month = month(Time)) %&gt;%\n    mutate(season = case_when(month %in% c(12, 1, 2) ~ \"Summer\"))\n\n__\nChallenge 8: Now, finish creating the season column using case_when for “Autumn”, “Winter” and “Spring”. You will need to add TRUE ~ season as the last argument so that the values we set for Summer in the previous chunk of code remain (i.e., you’re not writing over your summer values you just did).\n\n\nClick to see a solution\n\n\nsecchi_okaro &lt;- secchi_okaro %&gt;%\n    mutate(season = case_when(month %in% c(3, 4, 5) ~ \"Autumn\", month %in% c(6, 7,\n        8) ~ \"Winter\", month %in% c(9, 10, 11) ~ \"Spring\", TRUE ~ season))\n\n\n\nNow that we have our season column, let’s make a boxplot of the data by season to see if there are any obvious patterns. We will order the season as a factor first to make sure it plots in an order that makes sense.\n\nsecchi_okaro$season &lt;- factor(secchi_okaro$season, levels = c(\"Spring\", \"Summer\",\n    \"Autumn\", \"Winter\"))\n\nggplot(secchi_okaro, aes(x = season, y = secchi_m)) + geom_boxplot() + theme_bw() +\n    ylab(\"Secchi depth (m)\")\n\n\n\n\n\n\n\n\nOk, there are some clear differences between the seasons here. I have a feeling this ANOVA is gonna be interesting…We will use the function aov to run the ANOVA on the log-transformed column and summary to look at the results.\n\nanova_secchi_okaro &lt;- aov(log_secchi_m ~ season, data = secchi_okaro)\nsummary(anova_secchi_okaro)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nseason       3  7.668  2.5561   11.31 2.02e-06 ***\nResiduals   96 21.689  0.2259                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Pr(&gt;F) is very small, which tells us that there is a significant differences between seasons for Secchi depth.\nOne last thing we can check is which seasons are different from each other. We can run a Tukey test to see this.\n\ntukey_result &lt;- TukeyHSD(anova_secchi_okaro, conf.level = 0.95)\nprint(tukey_result)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log_secchi_m ~ season, data = secchi_okaro)\n\n$season\n                    diff          lwr         upr     p adj\nSummer-Spring  0.4014153  0.063513659  0.73931693 0.0130941\nAutumn-Spring  0.7483003  0.410398707  1.08620198 0.0000005\nWinter-Spring  0.3485858 -0.002656331  0.69982791 0.0525363\nAutumn-Summer  0.3468850 -0.011875594  0.70564569 0.0619260\nWinter-Summer -0.0528295 -0.424182046  0.31852304 0.9823365\nWinter-Autumn -0.3997145 -0.771067094 -0.02836201 0.0297440\n\nggplot(secchi_okaro, aes(x = season, y = log(secchi_m))) + geom_boxplot() + theme_bw() +\n    ylab(\"Secchi depth (m)\")\n\n\n\n\n\n\n\n\nLooking at the p-adj column, we can see which seasons have statistically significant differences. Let’s use a p-value cutoff of p &lt; 0.05 is considered significant. Here, we can see that winter-spring, autumn-summer, and winter-summer are not statistically different from each other. If we look back at our boxplots, this looks like a reasonable result given the differences between distributions of those seasons.\nNice job! You’ve made it to the end of this statistical lesson. If you still have time, you can try running an ANOVA across seasons in another lake. Come to us with any questions!"
  }
]